{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential Speedup imporvements\n",
    "\n",
    "path generation\n",
    "* remove any use of ROUND()\n",
    "* while returning paths, make sure all nodes in the paths are network nodes (no origin or destination nodes..).\n",
    "\n",
    "subgraph generation\n",
    "* consoliate + weight\n",
    "\n",
    "destination discovery\n",
    "* stop searching for origin nodes past 0.5 search radius\n",
    "* remove duplicate if statements\n",
    "\n",
    "\n",
    "update light graph\n",
    "* shorter list of segments to iterate over\n",
    "* one add one remove\n",
    "\n",
    "\n",
    "betweenness\n",
    "* remove decay calculations from inner loop.\n",
    "* vectorize betweenness calculation outside path loop\n",
    "* reduce calls to node_gdf.at[] by doing them when thwey're first possible, instead of inside path loop\n",
    "* vectorize destination probability calculations\n",
    "\n",
    "\n",
    "\n",
    "path generation\n",
    "* return paths as edges\n",
    "* remove len(neighbors) = 1 check, causes excessive calls to neighbors.\n",
    "* consolidate edge information retreval from graph.\n",
    "\n",
    "betweenness\n",
    "* eliminate edge list construction inside loop.\n",
    "\n",
    "parallel Betweenness:\n",
    "* Implement a queue for parallel processing\n",
    "* better reporting from quue progress\n",
    "\n",
    "Betweenness\n",
    "* update to accomidate queue feeding\n",
    "* implement controls for starting a job only when there is enough memory...\n",
    "* selete large variable prior to next job, so save for memory when waiting for memory to free up\n",
    "\n",
    "path generation:\n",
    "* remove edge sets, sacrificing time performance for memory savings\n",
    "* slightly more optimized scope neighbor, use set operations to filter neighbors in scope, instead of list comprehension with checking..\n",
    "* experament with dequeue for cheaper path appends...\n",
    "* significiant memory redunction by eliminating need to store \"targets remaining\", this might lead to reduction in cpu effeciency?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For next week:\n",
    "* Send rounaq wide table of counts\n",
    "* Once recieved, from lui, start a new digitization process in the new network\n",
    "* look at th eparks file situation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functionality List 3\n",
    "* set up a slideshow/diagram to show the relationship between different compoonents of the library.\n",
    "* Set up a notebook to go over estimating flows from one origin to one destination, Document all relevant settings\n",
    "* show how this generalizes to the pairings.csv, iterating over moodel settings, iterating over scenarios\n",
    "* Make a finalized github commit including testing script, betweenness flow notebook. \n",
    "* Deploy to workstation.\n",
    "* given the simulation results, and the censor locations, find a way to calibrate counts and generate beta coeffecients (Beirut, SOmerville, NYC)\n",
    "* enable Jupyter notebook server access from workstation.\n",
    "* Figure out a way to do the \"Cities\" dropbox folder, people can add data through dropbox, run simulations through Jupyter Server, get results through Dropbox\n",
    "\n",
    "## Map Cleaning\n",
    "https://www.dropbox.com/scl/fo/d6ugcyt3aanfb3yk4n9va/h?dl=0&rlkey=1c2giilvzoyygcxrlry7fscr9\n",
    "\n",
    "\n",
    "\n",
    "## Needed compatibility upgrades:\n",
    "* The query_bulk() method of the spatial index .sindex property is deprecated in favor of query() (#2823).\n",
    "* Make surr there is no direct use of pyGEOS (currently used in node-edge creation if tolerance=0) [ If you use PyGEOS directly and access an array of PyGEOS geometries using GeoSeries.values.data, you will need to make some changes to avoid code breakage.]  , a simple replacement of pygeos with shapely together with a change of gdf.geometry.values.data to gdf.geometry.values or analogous gdf.geometry.array should work\n",
    "\n",
    "\n",
    "## instaling an environment\n",
    "```\n",
    "Run Powershell as administrator\n",
    "\n",
    "conda create --name=madina_0_0_2 python\n",
    "conda activate madina_0_0_2\n",
    "conda config --env --add channels conda-forge\n",
    "conda config --env --set channel_priority strict\n",
    "conda install -f mkl\n",
    "conda install python=3 geopandas\n",
    "conda install pydeck\n",
    "conda install pyogrio\n",
    "conda install rtrees\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from madina.zonal.zonal import Zonal\n",
    "from madina.una.betweenness import parallel_betweenness, paralell_betweenness_exposure, betweenness_exposure\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "\n",
    "\n",
    "print (f\"{'test name':30s} | {'madina_flow_sum':15s} | {'Rhino_flow_sum':15s} | {'sum_diff':8s} | {'sum_smlr_%':10s} | {'sle_mean':8s} | {'sle_median':10s} | {'sle_max':8s} | {'sle>0.1%':8s} | {'sle>1.0%':8s} | {'sle>3.0%':8s} | {'sle>5.0%':8s} | {'time_spent':10s}\")\n",
    "#for test_case in os.listdir(\"Test Cases\"):\n",
    "for test_case in ['Harvard Square', 'Somerville']:   # 'Harvard Square',\n",
    "    start = time.time()\n",
    "    # TODO: Check OS compatibility, ensure this is compatible with Unix systems..\n",
    "    test_case_folder = \"Test Cases\" + \"\\\\\" + test_case + \"\\\\\"\n",
    "    test_config = pd.read_csv(test_case_folder + \"test_configs.csv\")\n",
    "    test_flows =  pd.read_csv(test_case_folder + \"test_flows.csv\")\n",
    "\n",
    "    if test_case == 'Somerville':\n",
    "        ready_tests = test_config.index[0:3]\n",
    "    else:\n",
    "        ready_tests = test_config.index\n",
    "\n",
    "    for test_idx in ready_tests:\n",
    "\n",
    "        harvard_square = Zonal()\n",
    "\n",
    "        harvard_square.load_layer(\n",
    "            layer_name='streets',\n",
    "            file_path=  test_case_folder + test_config.at[test_idx, 'Network_File']\n",
    "            )\n",
    "\n",
    "        harvard_square.load_layer(\n",
    "            layer_name=test_config.at[test_idx, 'Origin_Name'],\n",
    "            file_path= test_case_folder + test_config.at[test_idx, 'Origin_File']\n",
    "            )\n",
    "\n",
    "        harvard_square.load_layer(\n",
    "            layer_name=test_config.at[test_idx, 'Destination_Name'],\n",
    "            file_path= test_case_folder + test_config.at[test_idx, 'Destination_File']\n",
    "            )\n",
    "        \n",
    "\n",
    "        harvard_square.create_street_network(\n",
    "            source_layer='streets', \n",
    "            discard_redundant_edges=True,\n",
    "            split_redundant_edges=False,\n",
    "            node_snapping_tolerance=0.1,  # TODO: check for sensitivity... pick one as default snapping.\n",
    "            weight_attribute=test_config.at[test_idx, 'Network_Cost'] if test_config.at[test_idx, 'Network_Cost'] != \"Geometric\" else None\n",
    "        )\n",
    "\n",
    "        #print (f\"{harvard_square.network.edges.shape = }\")\n",
    "        #print (f\"{harvard_square.network.nodes.shape = }\")\n",
    "        #harvard_square.network.nodes,  harvard_square.network.edges = _discard_redundant_edges(harvard_square.network.nodes, harvard_square.network.edges)\n",
    "        #print (f\"{harvard_square.network.edges.shape = }\")\n",
    "        #print (f\"{harvard_square.network.nodes.shape = }\")\n",
    "\n",
    "        harvard_square.insert_node(\n",
    "            layer_name=test_config.at[test_idx, 'Origin_Name'], \n",
    "            label='origin', \n",
    "            weight_attribute=test_config.at[test_idx, 'Origin_Weight'] if test_config.at[test_idx, 'Origin_Weight'] != \"Count\" else None\n",
    "        )\n",
    "        harvard_square.insert_node(\n",
    "            layer_name=test_config.at[test_idx, 'Destination_Name'], \n",
    "            label='destination', \n",
    "            weight_attribute=test_config.at[test_idx, 'Destination_Weight'] if test_config.at[test_idx, 'Destination_Weight'] != \"Count\" else None\n",
    "        )\n",
    "\n",
    "        harvard_square.create_graph()\n",
    "\n",
    "        node_gdf = harvard_square.network.nodes\n",
    "        origin_gdf = node_gdf[node_gdf['type'] == 'origin']\n",
    "\n",
    "        harvard_square.network.nodes[\"original_weight\"] = harvard_square.network.nodes[\"weight\"]\n",
    "\n",
    "        harvard_square.network.turn_penalty_amount = test_config.at[test_idx, 'Turn_Penalty']\n",
    "        harvard_square.network.turn_threshold_degree = test_config.at[test_idx, 'Turn_Threshold']\n",
    "\n",
    "        if test_config.at[test_idx, 'Elastic_Weights']:\n",
    "            '''\n",
    "            harvard_square.network.nodes[\"weight\"] = harvard_square.network.nodes[\"original_weight\"]\n",
    "            get_elastic_weight(\n",
    "                harvard_square.network,\n",
    "                search_radius=test_config.at[test_idx, 'Radius'],\n",
    "                detour_ratio=test_config.at[test_idx, 'Detour'],\n",
    "                beta=test_config.at[test_idx, 'Beta'],\n",
    "                decay=True, #test_config.at[test_idx, 'Decay'],\n",
    "                #turn_penalty=test_config.at[test_idx, 'Turns'],\n",
    "                turn_penalty=False,\n",
    "            )\n",
    "            for o_idx in origin_gdf.index:\n",
    "                harvard_square.network.nodes.at[o_idx, 'weight'] =  harvard_square.network.nodes.at[o_idx, 'elastic_weight']\n",
    "            '''\n",
    "            continue\n",
    "        '''\n",
    "        return_dict = parallel_betweenness(\n",
    "            harvard_square.network,\n",
    "            search_radius=test_config.at[test_idx, 'Radius'],\n",
    "            detour_ratio=test_config.at[test_idx, 'Detour'],\n",
    "            decay=test_config.at[test_idx, 'Decay'], #if test['Elastic weights'] else True,\n",
    "            decay_method=test_config.at[test_idx, 'Decay_Mode'],  # \"power\", \"exponent\"\n",
    "            beta=test_config.at[test_idx, 'Beta'],\n",
    "            path_detour_penalty='equal', # \"equal\",  # \"power\", \"exponent\", \"equal\"\n",
    "            origin_weights=False if type(test_config.at[test_idx, 'Origin_Weight']) != str else True,\n",
    "            closest_destination=test_config.at[test_idx, 'Closest_Destination'],\n",
    "            destination_weights=False if type(test_config.at[test_idx, 'Destination_Weight']) != str  else True,    #or (test['Elastic weights'])\n",
    "            # perceived_distance=False,\n",
    "            num_cores=6,\n",
    "            light_graph=True,\n",
    "            turn_penalty=test_config.at[test_idx, 'Turns'],\n",
    "        )\n",
    "        \n",
    "        '''\n",
    "        harvard_square.network.max_chunck_size = 50\n",
    "        harvard_square.network.chunking_method = 'pizza_chunks' # ['no_chunking', 'cocentric-chunks', 'random_chunks', 'pizza_chunks']\n",
    "        return_dict = paralell_betweenness_exposure(\n",
    "            harvard_square,\n",
    "            search_radius=test_config.at[test_idx,'Radius'],\n",
    "            detour_ratio=test_config.at[test_idx,'Detour'],\n",
    "            decay=False if test_config.at[test_idx,'Elastic_Weights'] else test_config.at[test_idx,'Decay'],  # elastic weight already reduces origin weight factoring in decay. if this pairing uses elastic weights, don't decay again,\n",
    "            decay_method=test_config.at[test_idx,'Decay_Mode'],\n",
    "            beta=test_config.at[test_idx,'Beta'],\n",
    "            num_cores=3,\n",
    "            path_detour_penalty='equal', # \"power\" | \"exponent\" | \"equal\"\n",
    "            closest_destination=test_config.at[test_idx,'Closest_Destination'],\n",
    "            elastic_weight=test_config.at[test_idx,'Elastic_Weights'],\n",
    "            turn_penalty=test_config.at[test_idx,'Turns'],\n",
    "            path_exposure_attribute=None,\n",
    "            return_path_record=False, \n",
    "            destniation_cap=None\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        simulated_sum_of_flow = return_dict['edge_gdf']['betweenness'].sum()\n",
    "        test_flow = test_flows[test_config.at[test_idx, 'Flow_Name']].sum()\n",
    "\n",
    "\n",
    "        ## create segment level comparison\n",
    "        # creating connector lines\n",
    "\n",
    "        import shapely.geometry as geo\n",
    "        simulated_betweenness = return_dict['edge_gdf'][['betweenness', 'parent_street_id']].rename(columns={\"betweenness\": \"simulated_betweenness\"}).drop_duplicates(subset=['parent_street_id']).set_index(\"parent_street_id\")\n",
    "        simulated_betweenness = harvard_square.layers[\"streets\"].gdf[[\"geometry\", \"__GUID\"]].join(simulated_betweenness).set_index(\"__GUID\")\n",
    "\n",
    "        test_name = test_config.at[test_idx, 'Flow_Name']\n",
    "        test_betweenness = test_flows[['__GUID', test_name]].set_index(\"__GUID\").rename(columns = {test_name: \"test_flow\"})\n",
    "\n",
    "\n",
    "        comparison = simulated_betweenness.join(test_betweenness)\n",
    "        comparison[\"difference\"] = comparison[\"simulated_betweenness\"] - comparison[\"test_flow\"]\n",
    "        comparison[\"difference_pct\"] = abs(comparison[\"difference\"]) / comparison[\"simulated_betweenness\"] *100\n",
    "        # segment level error\n",
    "        sle = comparison[~comparison[\"difference_pct\"].isna()]['difference_pct']\n",
    "\n",
    "        \n",
    "        node_gdf = harvard_square.network.nodes\n",
    "        edge_gdf = harvard_square.network.edges\n",
    "\n",
    "\n",
    "        origin_nodes = node_gdf[node_gdf['type'] == 'origin']\n",
    "        '''\n",
    "        origin_layer = harvard_square.layers[test_config.at[test_idx, 'Origin_Name']].gdf\n",
    "        origin_joined = origin_layer.join(origin_nodes.set_index('source_id'),lsuffix='_origin')\n",
    "        origin_joined['connector_line'] = origin_joined.apply(lambda x:geo.LineString([x['geometry'], x[\"geometry_origin\"]]), axis=1)\n",
    "        origin_joined[\"geometry\"] = origin_joined['connector_line']\n",
    "\n",
    "\n",
    "        destination_layer = harvard_square.layers[test_config.at[test_idx, 'Destination_Name']].gdf\n",
    "        destination_nodes = node_gdf[node_gdf['type'] == 'destination']\n",
    "        destination_joined = destination_layer.join(destination_nodes.set_index('source_id'),lsuffix='_destination')\n",
    "        destination_joined['connector_line'] = destination_joined.apply(lambda x:geo.LineString([x['geometry'], x[\"geometry_destination\"]]), axis=1)\n",
    "        destination_joined[\"geometry\"] = destination_joined['connector_line']\n",
    "\n",
    "\n",
    "        #print (f\"{origin_nodes.shape = }\\t{destination_nodes.shape = }\")\n",
    "\n",
    "        streets = harvard_square.layers[\"streets\"].gdf \n",
    "        network_file = gpd.read_file(test_case_folder + test_config.at[test_idx, 'Network_File'], engine='pyogrio')\n",
    "\n",
    "\n",
    "        flow_difference = comparison[\n",
    "            ((comparison['test_flow' ] > 0) & (comparison['simulated_betweenness'] == 0)) | \n",
    "            ((comparison['test_flow' ] == 0) & (comparison['simulated_betweenness'] > 0))\n",
    "        ]\n",
    "        harvard_square.create_map(\n",
    "            [\n",
    "                #{'gdf': streets[streets[test_config.at[test_idx, 'Network_Cost']] > 0], 'color': [255, 255, 255], 'text': test_config.at[test_idx, 'Network_Cost']},\n",
    "                {'gdf': streets, 'color': [100, 100, 100], 'opacity': 0.1},\n",
    "                {'gdf': edge_gdf[edge_gdf['betweenness'] > 0], 'color': ['125, 125, 0'], 'text': 'betweenness', 'opacity': 0.2},\n",
    "                #{'gdf': comparison[abs(comparison[\"difference\"]) > 0.01], 'color_by_attribute': 'difference', 'color_method': 'gradient', 'text': 'difference'},\n",
    "                {'gdf': comparison[(comparison[\"difference_pct\"] >= 0.1) & (comparison[\"difference_pct\"] < 100)], 'color_by_attribute': 'difference_pct', 'color_method': 'gradient', 'text': 'difference_pct'},\n",
    "                {'gdf': edge_gdf[edge_gdf['snapped'] == True], 'color': [255, 0, 255], 'opacity': 0.2},\n",
    "                {'gdf': network_file[network_file[\"geometry\"].geom_type == 'Polygon'], 'color': [125, 0, 125], 'text': '__GUID'},\n",
    "                {'gdf': flow_difference, 'color': [255, 255, 0] , 'text': 'difference'},        \n",
    "                #{'gdf': comparison[comparison['difference'] != 0], 'color_by_attribute': 'difference', 'color_method': 'gradient', 'text': 'difference', 'opacity':  0.1},\n",
    "                {'gdf': origin_layer, 'color': [0, 0, 255]},\n",
    "                {'gdf': origin_joined[['geometry']], 'color': [0, 0, 255]},\n",
    "                {'gdf': destination_layer, 'color': [255, 0, 0]},\n",
    "                {'gdf': destination_joined[['geometry']], 'color': [255, 0, 0]},\n",
    "                {'gdf': harvard_square.network.nodes[['geometry', 'type', 'weight']].reset_index(), 'color': [255, 0, 255], 'text': 'id'},\n",
    "            ],\n",
    "            save_as=\"Test Cases\\\\\" + test_case + '\\\\'  + test_name + \"._difference_map.html\"\n",
    "        )\n",
    "        '''\n",
    "        \n",
    "        #print (test_config.loc[test_idx])\n",
    "        print (f\"{test_config.at[test_idx, 'Flow_Name'][:30]:30s} | {simulated_sum_of_flow:15.2f} | {test_flow:15.2f} | {simulated_sum_of_flow - test_flow:8.2f} | {1-(simulated_sum_of_flow - test_flow)/ test_flow:10.2%} | {sle.mean():7.4f}% | {sle.median():9.4f}% | {sle.max():7.4f}% | {sle[sle > 0.1].count():8} | {sle[sle > 1.0].count():8} | {sle[sle > 3.0].count():8} | {sle[sle > 5.0].count():8} | {time.time() - start: 10.5f}\")\n",
    "        #print (\"DOne Case...\")\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping track of visited destinations\n",
    "test name                      | madina_flow_sum | Rhino_flow_sum  | sum_diff | sum_smlr_% | sle_mean | sle_median | sle_max  | sle>0.1% | sle>1.0% | sle>3.0% | sle>5.0% | time_spent\n",
    "_Betweenness1                  |          702.07 |          702.07 |    -0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |    4.27237\n",
    "_Betweenness2                  |        19025.63 |        19025.63 |     0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |    7.92910\n",
    "_Betweenness3                  |        19214.00 |        19214.00 |     0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |   11.60755\n",
    "_Betweenness4                  |        20853.18 |        20853.18 |    -0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |   15.20730\n",
    "Somerville_Bus_Subway_Geometri |          700.29 |          713.14 |   -12.84 |    101.80% |  2.3488% |    2.2893% |  5.0852% |      266 |      232 |       96 |        2 |   12.55345\n",
    "Somerville_Bus_Subway          |          730.95 |          743.95 |   -13.00 |    101.75% |  2.4117% |    2.2917% |  5.1912% |      285 |      251 |      103 |        8 |   25.87746\n",
    "Somerville_Homes_Subway        |       368786.93 |       389383.31 | -20596.38 |    105.29% |     inf% |    6.9536% |     inf% |     2640 |     2565 |     2355 |     1966 |  277.37418\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_dict['origin_gdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "px.scatter(\n",
    "    return_dict['origin_gdf'], \n",
    "    x='reach', \n",
    "    y='path_generation_time'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "def profile_new():\n",
    "    origin_gdf = origin_nodes\n",
    "    num_cores = 1\n",
    "    with mp.Manager() as manager:\n",
    "        #create aNDfill queue\n",
    "        origin_queue = manager.Queue()\n",
    "        for o_idx in origin_gdf.index[:]:\n",
    "            origin_queue.put(o_idx)\n",
    "        for core_index in range(num_cores):\n",
    "            origin_queue.put(\"done\")\n",
    "\n",
    "\n",
    "        return_dict = betweenness_exposure(\n",
    "            harvard_square,\n",
    "            core_index=1,\n",
    "            origin_queue=origin_queue,\n",
    "            #origins=origin_gdf.iloc[:1000],\n",
    "            search_radius=test_config.at[test_idx,'Radius'],\n",
    "            detour_ratio=test_config.at[test_idx,'Detour'],\n",
    "            decay=False if test_config.at[test_idx,'Elastic_Weights'] else test_config.at[test_idx,'Decay'],\n",
    "            beta=test_config.at[test_idx,'Beta'],\n",
    "            decay_method=test_config.at[test_idx,'Decay_Mode'],\n",
    "            path_detour_penalty='equal',\n",
    "            closest_destination=test_config.at[test_idx,'Closest_Destination'],\n",
    "            elastic_weight=test_config.at[test_idx,'Elastic_Weights'],\n",
    "            turn_penalty=test_config.at[test_idx,'Turns'],\n",
    "            path_exposure_attribute=None,\n",
    "            return_path_record=False, \n",
    "            destniation_cap=None\n",
    "    )\n",
    "    return return_dict\n",
    "x = profile_new()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "madina_env_latest_updates",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
