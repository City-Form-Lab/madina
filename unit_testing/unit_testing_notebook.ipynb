{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential Speedup imporvements\n",
    "\n",
    "path generation\n",
    "* remove any use of ROUND()\n",
    "* while returning paths, make sure all nodes in the paths are network nodes (no origin or destination nodes..).\n",
    "\n",
    "subgraph generation\n",
    "* consoliate + weight\n",
    "\n",
    "destination discovery\n",
    "* stop searching for origin nodes past 0.5 search radius\n",
    "* remove duplicate if statements\n",
    "\n",
    "\n",
    "update light graph\n",
    "* shorter list of segments to iterate over\n",
    "* one add one remove\n",
    "\n",
    "\n",
    "betweenness\n",
    "* remove decay calculations from inner loop.\n",
    "* vectorize betweenness calculation outside path loop\n",
    "* reduce calls to node_gdf.at[] by doing them when thwey're first possible, instead of inside path loop\n",
    "* vectorize destination probability calculations\n",
    "\n",
    "\n",
    "\n",
    "path generation\n",
    "* return paths as edges\n",
    "* remove len(neighbors) = 1 check, causes excessive calls to neighbors.\n",
    "* consolidate edge information retreval from graph.\n",
    "\n",
    "betweenness\n",
    "* eliminate edge list construction inside loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For next week:\n",
    "* Send rounaq wide table of counts\n",
    "* Once recieved, from lui, start a new digitization process in the new network\n",
    "* look at th eparks file situation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functionality List 3\n",
    "* set up a slideshow/diagram to show the relationship between different compoonents of the library.\n",
    "* Set up a notebook to go over estimating flows from one origin to one destination, Document all relevant settings\n",
    "* show how this generalizes to the pairings.csv, iterating over moodel settings, iterating over scenarios\n",
    "* Make a finalized github commit including testing script, betweenness flow notebook. \n",
    "* Deploy to workstation.\n",
    "* given the simulation results, and the censor locations, find a way to calibrate counts and generate beta coeffecients (Beirut, SOmerville, NYC)\n",
    "* enable Jupyter notebook server access from workstation.\n",
    "* Figure out a way to do the \"Cities\" dropbox folder, people can add data through dropbox, run simulations through Jupyter Server, get results through Dropbox\n",
    "\n",
    "## Map Cleaning\n",
    "https://www.dropbox.com/scl/fo/d6ugcyt3aanfb3yk4n9va/h?dl=0&rlkey=1c2giilvzoyygcxrlry7fscr9\n",
    "\n",
    "\n",
    "\n",
    "## Needed compatibility upgrades:\n",
    "* The query_bulk() method of the spatial index .sindex property is deprecated in favor of query() (#2823).\n",
    "* Make surr there is no direct use of pyGEOS (currently used in node-edge creation if tolerance=0) [ If you use PyGEOS directly and access an array of PyGEOS geometries using GeoSeries.values.data, you will need to make some changes to avoid code breakage.]  , a simple replacement of pygeos with shapely together with a change of gdf.geometry.values.data to gdf.geometry.values or analogous gdf.geometry.array should work\n",
    "\n",
    "\n",
    "## instaling an environment\n",
    "```\n",
    "Run Powershell as administrator\n",
    "\n",
    "conda create --name=madina_0_0_2 python\n",
    "conda activate madina_0_0_2\n",
    "conda config --env --add channels conda-forge\n",
    "conda config --env --set channel_priority strict\n",
    "conda install -f mkl\n",
    "conda install python=3 geopandas\n",
    "conda install pydeck\n",
    "conda install pyogrio\n",
    "conda install rtrees\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from madina.zonal.zonal import Zonal\n",
    "from madina.una.betweenness import parallel_betweenness, paralell_betweenness_exposure, betweenness_exposure\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "\n",
    "\n",
    "print (f\"{'test name':30s} | {'madina_flow_sum':15s} | {'Rhino_flow_sum':15s} | {'sum_diff':8s} | {'sum_smlr_%':10s} | {'sle_mean':8s} | {'sle_median':10s} | {'sle_max':8s} | {'sle>0.1%':8s} | {'sle>1.0%':8s} | {'sle>3.0%':8s} | {'sle>5.0%':8s} | {'time_spent':10s}\")\n",
    "#for test_case in os.listdir(\"Test Cases\"):\n",
    "for test_case in ['Harvard Square', 'Somerville']:   # 'Harvard Square',\n",
    "    start = time.time()\n",
    "    # TODO: Check OS compatibility, ensure this is compatible with Unix systems..\n",
    "    test_case_folder = \"Test Cases\" + \"\\\\\" + test_case + \"\\\\\"\n",
    "    test_config = pd.read_csv(test_case_folder + \"test_configs.csv\")\n",
    "    test_flows =  pd.read_csv(test_case_folder + \"test_flows.csv\")\n",
    "\n",
    "    if test_case == 'Somerville':\n",
    "        ready_tests = test_config.index[0:3]\n",
    "    else:\n",
    "        ready_tests = test_config.index\n",
    "\n",
    "    for test_idx in ready_tests:\n",
    "\n",
    "        harvard_square = Zonal()\n",
    "\n",
    "        harvard_square.load_layer(\n",
    "            layer_name='streets',\n",
    "            file_path=  test_case_folder + test_config.at[test_idx, 'Network_File']\n",
    "            )\n",
    "\n",
    "        harvard_square.load_layer(\n",
    "            layer_name=test_config.at[test_idx, 'Origin_Name'],\n",
    "            file_path= test_case_folder + test_config.at[test_idx, 'Origin_File']\n",
    "            )\n",
    "\n",
    "        harvard_square.load_layer(\n",
    "            layer_name=test_config.at[test_idx, 'Destination_Name'],\n",
    "            file_path= test_case_folder + test_config.at[test_idx, 'Destination_File']\n",
    "            )\n",
    "        \n",
    "\n",
    "        harvard_square.create_street_network(\n",
    "            source_layer='streets', \n",
    "            discard_redundant_edges=True,\n",
    "            split_redundant_edges=False,\n",
    "            node_snapping_tolerance=0.1,  # TODO: check for sensitivity... pick one as default snapping.\n",
    "            weight_attribute=test_config.at[test_idx, 'Network_Cost'] if test_config.at[test_idx, 'Network_Cost'] != \"Geometric\" else None\n",
    "        )\n",
    "\n",
    "        #print (f\"{harvard_square.network.edges.shape = }\")\n",
    "        #print (f\"{harvard_square.network.nodes.shape = }\")\n",
    "        #harvard_square.network.nodes,  harvard_square.network.edges = _discard_redundant_edges(harvard_square.network.nodes, harvard_square.network.edges)\n",
    "        #print (f\"{harvard_square.network.edges.shape = }\")\n",
    "        #print (f\"{harvard_square.network.nodes.shape = }\")\n",
    "\n",
    "        harvard_square.insert_node(\n",
    "            layer_name=test_config.at[test_idx, 'Origin_Name'], \n",
    "            label='origin', \n",
    "            weight_attribute=test_config.at[test_idx, 'Origin_Weight'] if test_config.at[test_idx, 'Origin_Weight'] != \"Count\" else None\n",
    "        )\n",
    "        harvard_square.insert_node(\n",
    "            layer_name=test_config.at[test_idx, 'Destination_Name'], \n",
    "            label='destination', \n",
    "            weight_attribute=test_config.at[test_idx, 'Destination_Weight'] if test_config.at[test_idx, 'Destination_Weight'] != \"Count\" else None\n",
    "        )\n",
    "\n",
    "        harvard_square.create_graph()\n",
    "\n",
    "        node_gdf = harvard_square.network.nodes\n",
    "        origin_gdf = node_gdf[node_gdf['type'] == 'origin']\n",
    "\n",
    "        harvard_square.network.nodes[\"original_weight\"] = harvard_square.network.nodes[\"weight\"]\n",
    "\n",
    "        harvard_square.network.turn_penalty_amount = test_config.at[test_idx, 'Turn_Penalty']\n",
    "        harvard_square.network.turn_threshold_degree = test_config.at[test_idx, 'Turn_Threshold']\n",
    "\n",
    "        if test_config.at[test_idx, 'Elastic_Weights']:\n",
    "            '''\n",
    "            harvard_square.network.nodes[\"weight\"] = harvard_square.network.nodes[\"original_weight\"]\n",
    "            get_elastic_weight(\n",
    "                harvard_square.network,\n",
    "                search_radius=test_config.at[test_idx, 'Radius'],\n",
    "                detour_ratio=test_config.at[test_idx, 'Detour'],\n",
    "                beta=test_config.at[test_idx, 'Beta'],\n",
    "                decay=True, #test_config.at[test_idx, 'Decay'],\n",
    "                #turn_penalty=test_config.at[test_idx, 'Turns'],\n",
    "                turn_penalty=False,\n",
    "            )\n",
    "            for o_idx in origin_gdf.index:\n",
    "                harvard_square.network.nodes.at[o_idx, 'weight'] =  harvard_square.network.nodes.at[o_idx, 'elastic_weight']\n",
    "            '''\n",
    "            continue\n",
    "        '''\n",
    "        return_dict = parallel_betweenness(\n",
    "            harvard_square.network,\n",
    "            search_radius=test_config.at[test_idx, 'Radius'],\n",
    "            detour_ratio=test_config.at[test_idx, 'Detour'],\n",
    "            decay=test_config.at[test_idx, 'Decay'], #if test['Elastic weights'] else True,\n",
    "            decay_method=test_config.at[test_idx, 'Decay_Mode'],  # \"power\", \"exponent\"\n",
    "            beta=test_config.at[test_idx, 'Beta'],\n",
    "            path_detour_penalty='equal', # \"equal\",  # \"power\", \"exponent\", \"equal\"\n",
    "            origin_weights=False if type(test_config.at[test_idx, 'Origin_Weight']) != str else True,\n",
    "            closest_destination=test_config.at[test_idx, 'Closest_Destination'],\n",
    "            destination_weights=False if type(test_config.at[test_idx, 'Destination_Weight']) != str  else True,    #or (test['Elastic weights'])\n",
    "            # perceived_distance=False,\n",
    "            num_cores=6,\n",
    "            light_graph=True,\n",
    "            turn_penalty=test_config.at[test_idx, 'Turns'],\n",
    "        )\n",
    "        \n",
    "        '''\n",
    "        return_dict = paralell_betweenness_exposure(\n",
    "            harvard_square,\n",
    "            search_radius=test_config.at[test_idx,'Radius'],\n",
    "            detour_ratio=test_config.at[test_idx,'Detour'],\n",
    "            decay=False if test_config.at[test_idx,'Elastic_Weights'] else test_config.at[test_idx,'Decay'],  # elastic weight already reduces origin weight factoring in decay. if this pairing uses elastic weights, don't decay again,\n",
    "            decay_method=test_config.at[test_idx,'Decay_Mode'],\n",
    "            beta=test_config.at[test_idx,'Beta'],\n",
    "            num_cores=3,\n",
    "            path_detour_penalty='equal', # \"power\" | \"exponent\" | \"equal\"\n",
    "            closest_destination=test_config.at[test_idx,'Closest_Destination'],\n",
    "            elastic_weight=test_config.at[test_idx,'Elastic_Weights'],\n",
    "            turn_penalty=test_config.at[test_idx,'Turns'],\n",
    "            path_exposure_attribute=None,\n",
    "            return_path_record=False, \n",
    "            destniation_cap=None\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        simulated_sum_of_flow = return_dict['edge_gdf']['betweenness'].sum()\n",
    "        test_flow = test_flows[test_config.at[test_idx, 'Flow_Name']].sum()\n",
    "\n",
    "\n",
    "        ## create segment level comparison\n",
    "        # creating connector lines\n",
    "\n",
    "        import shapely.geometry as geo\n",
    "        simulated_betweenness = return_dict['edge_gdf'][['betweenness', 'parent_street_id']].rename(columns={\"betweenness\": \"simulated_betweenness\"}).drop_duplicates(subset=['parent_street_id']).set_index(\"parent_street_id\")\n",
    "        simulated_betweenness = harvard_square.layers[\"streets\"].gdf[[\"geometry\", \"__GUID\"]].join(simulated_betweenness).set_index(\"__GUID\")\n",
    "\n",
    "        test_name = test_config.at[test_idx, 'Flow_Name']\n",
    "        test_betweenness = test_flows[['__GUID', test_name]].set_index(\"__GUID\").rename(columns = {test_name: \"test_flow\"})\n",
    "\n",
    "\n",
    "        comparison = simulated_betweenness.join(test_betweenness)\n",
    "        comparison[\"difference\"] = comparison[\"simulated_betweenness\"] - comparison[\"test_flow\"]\n",
    "        comparison[\"difference_pct\"] = abs(comparison[\"difference\"]) / comparison[\"simulated_betweenness\"] *100\n",
    "        # segment level error\n",
    "        sle = comparison[~comparison[\"difference_pct\"].isna()]['difference_pct']\n",
    "\n",
    "        \n",
    "        node_gdf = harvard_square.network.nodes\n",
    "        edge_gdf = harvard_square.network.edges\n",
    "\n",
    "\n",
    "        origin_layer = harvard_square.layers[test_config.at[test_idx, 'Origin_Name']].gdf\n",
    "        origin_nodes = node_gdf[node_gdf['type'] == 'origin']\n",
    "        origin_joined = origin_layer.join(origin_nodes.set_index('source_id'),lsuffix='_origin')\n",
    "        origin_joined['connector_line'] = origin_joined.apply(lambda x:geo.LineString([x['geometry'], x[\"geometry_origin\"]]), axis=1)\n",
    "        origin_joined[\"geometry\"] = origin_joined['connector_line']\n",
    "\n",
    "\n",
    "        destination_layer = harvard_square.layers[test_config.at[test_idx, 'Destination_Name']].gdf\n",
    "        destination_nodes = node_gdf[node_gdf['type'] == 'destination']\n",
    "        destination_joined = destination_layer.join(destination_nodes.set_index('source_id'),lsuffix='_destination')\n",
    "        destination_joined['connector_line'] = destination_joined.apply(lambda x:geo.LineString([x['geometry'], x[\"geometry_destination\"]]), axis=1)\n",
    "        destination_joined[\"geometry\"] = destination_joined['connector_line']\n",
    "\n",
    "\n",
    "        #print (f\"{origin_nodes.shape = }\\t{destination_nodes.shape = }\")\n",
    "\n",
    "        streets = harvard_square.layers[\"streets\"].gdf \n",
    "        network_file = gpd.read_file(test_case_folder + test_config.at[test_idx, 'Network_File'], engine='pyogrio')\n",
    "\n",
    "\n",
    "        flow_difference = comparison[\n",
    "            ((comparison['test_flow' ] > 0) & (comparison['simulated_betweenness'] == 0)) | \n",
    "            ((comparison['test_flow' ] == 0) & (comparison['simulated_betweenness'] > 0))\n",
    "        ]\n",
    "        '''\n",
    "        harvard_square.create_map(\n",
    "            [\n",
    "                #{'gdf': streets[streets[test_config.at[test_idx, 'Network_Cost']] > 0], 'color': [255, 255, 255], 'text': test_config.at[test_idx, 'Network_Cost']},\n",
    "                {'gdf': streets, 'color': [100, 100, 100], 'opacity': 0.1},\n",
    "                {'gdf': edge_gdf[edge_gdf['betweenness'] > 0], 'color': ['125, 125, 0'], 'text': 'betweenness', 'opacity': 0.2},\n",
    "                #{'gdf': comparison[abs(comparison[\"difference\"]) > 0.01], 'color_by_attribute': 'difference', 'color_method': 'gradient', 'text': 'difference'},\n",
    "                {'gdf': comparison[(comparison[\"difference_pct\"] >= 0.1) & (comparison[\"difference_pct\"] < 100)], 'color_by_attribute': 'difference_pct', 'color_method': 'gradient', 'text': 'difference_pct'},\n",
    "                {'gdf': edge_gdf[edge_gdf['snapped'] == True], 'color': [255, 0, 255], 'opacity': 0.2},\n",
    "                {'gdf': network_file[network_file[\"geometry\"].geom_type == 'Polygon'], 'color': [125, 0, 125], 'text': '__GUID'},\n",
    "                {'gdf': flow_difference, 'color': [255, 255, 0] , 'text': 'difference'},        \n",
    "                #{'gdf': comparison[comparison['difference'] != 0], 'color_by_attribute': 'difference', 'color_method': 'gradient', 'text': 'difference', 'opacity':  0.1},\n",
    "                {'gdf': origin_layer, 'color': [0, 0, 255]},\n",
    "                {'gdf': origin_joined[['geometry']], 'color': [0, 0, 255]},\n",
    "                {'gdf': destination_layer, 'color': [255, 0, 0]},\n",
    "                {'gdf': destination_joined[['geometry']], 'color': [255, 0, 0]},\n",
    "                {'gdf': harvard_square.network.nodes[['geometry', 'type', 'weight']].reset_index(), 'color': [255, 0, 255], 'text': 'id'},\n",
    "            ],\n",
    "            save_as=\"Test Cases\\\\\" + test_case + '\\\\'  + test_name + \"._difference_map.html\"\n",
    "        )\n",
    "        '''\n",
    "        \n",
    "        #print (test_config.loc[test_idx])\n",
    "        print (f\"{test_config.at[test_idx, 'Flow_Name'][:30]:30s} | {simulated_sum_of_flow:15.2f} | {test_flow:15.2f} | {simulated_sum_of_flow - test_flow:8.2f} | {1-(simulated_sum_of_flow - test_flow)/ test_flow:10.2%} | {sle.mean():7.4f}% | {sle.median():9.4f}% | {sle.max():7.4f}% | {sle[sle > 0.1].count():8} | {sle[sle > 1.0].count():8} | {sle[sle > 3.0].count():8} | {sle[sle > 5.0].count():8} | {time.time() - start: 10.5f}\")\n",
    "        #print (\"DOne Case...\")\n",
    "        #break\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vectorized exposure betweennes swith segment ids, removing len 1 neighbor check, consolidating edge information retreval\n",
    "test name                      | madina_flow_sum | Rhino_flow_sum  | sum_diff | sum_smlr_% | sle_mean | sle_median | sle_max  | sle>0.1% | sle>1.0% | sle>3.0% | sle>5.0% | time_spent\n",
    "_Betweenness1                  |          702.07 |          702.07 |    -0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |    3.87253\n",
    "_Betweenness2                  |        19025.63 |        19025.63 |     0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |    6.86009\n",
    "_Betweenness3                  |        19214.00 |        19214.00 |     0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |    9.66996\n",
    "_Betweenness4                  |        20853.18 |        20853.18 |    -0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |   12.36717\n",
    "Somerville_Bus_Subway_Geometri |          700.29 |          713.14 |   -12.84 |    101.80% |  2.3488% |    2.2893% |  5.0852% |      266 |      232 |       96 |        2 |   16.73959\n",
    "Somerville_Bus_Subway          |          730.95 |          743.95 |   -13.00 |    101.75% |  2.4117% |    2.2917% |  5.1912% |      285 |      251 |      103 |        8 |   34.42406\n",
    "Somerville_Homes_Subway        |       368161.49 |       389383.31 | -21221.81 |    105.45% |     inf% |    7.2526% |     inf% |     2665 |     2631 |     2521 |     2216 |  319.56011\n",
    "## vectorized exposure betweennes swith segment ids\n",
    "test name                      | madina_flow_sum | Rhino_flow_sum  | sum_diff | sum_smlr_% | sle_mean | sle_median | sle_max  | sle>0.1% | sle>1.0% | sle>3.0% | sle>5.0% | time_spent\n",
    "_Betweenness1                  |          702.07 |          702.07 |    -0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |    3.75356\n",
    "_Betweenness2                  |        19025.63 |        19025.63 |     0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |    6.62444\n",
    "_Betweenness3                  |        19214.00 |        19214.00 |     0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |   10.33437\n",
    "_Betweenness4                  |        20853.18 |        20853.18 |    -0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |   14.16261\n",
    "omerville_Bus_Subway_Geometri |          700.29 |          713.14 |   -12.84 |    101.80% |  2.3488% |    2.2893% |  5.0852% |      266 |      232 |       96 |        2 |   18.64562\n",
    "Somerville_Bus_Subway          |          730.95 |          743.95 |   -13.00 |    101.75% |  2.4117% |    2.   2917% |  5.1912% |      285 |      251 |      103 |        8 |   37.69253\n",
    "Somerville_Homes_Subway        |       368161.49 |       389383.31 | -21221.81 |    105.45% |     inf% |    7.2526% |     inf% |     2665 |     2631 |     2521 |     2216 |  324.89220\n",
    "\n",
    "\n",
    "\n",
    "# One betweenness, with all the new network fixes... (THIS IS ON ^ CORES> DO NOT COMPARE TIME)\n",
    "test name                      | madina_flow_sum | Rhino_flow_sum  | sum_diff | sum_smlr_% | sle_mean | sle_median | sle_max  | sle>0.1% | sle>1.0% | sle>3.0% | sle>5.0% | time_spent\n",
    "_Betweenness1                  |          704.85 |          702.07 |     2.78 |     99.60% |  0.0376% |    0.0000% |  2.4893% |        2 |        2 |        0 |        0 |    4.52267\n",
    "_Betweenness2                  |        19116.27 |        19025.63 |    90.64 |     99.52% |  0.0439% |    0.0000% |  3.1837% |        2 |        2 |        1 |        0 |    8.24166\n",
    "_Betweenness3                  |        19299.00 |        19214.00 |    85.00 |     99.56% |  0.0295% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0 |   12.25564\n",
    "_Betweenness4                  |        20938.18 |        20853.18 |    85.00 |     99.59% |  0.0206% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0 |   16.16196\n",
    "Somerville_Bus_Subway_Geometri |          709.44 |          713.14 |    -3.69 |    100.52% |  0.5027% |    0.1954% |  3.9604% |      149 |       62 |        3 |        0 |   19.41082\n",
    "Somerville_Bus_Subway          |          740.19 |          743.95 |    -3.76 |    100.51% |  0.5074% |    0.2539% |  3.9604% |      173 |       39 |        3 |        0 |   38.43126\n",
    "Somerville_Homes_Subway        |       377967.80 |       389383.31 | -11415.51 |    102.93% |     inf% |    3.8514% |     inf% |     2579 |     2265 |     1591 |     1001 |  264.22996\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Vectorized betweenness, fix for unrelevant destinations ruining graph\n",
    "test name                      | madina_flow_sum | Rhino_flow_sum  | sum_diff | sum_smlr_% | sle_mean | sle_median | sle_max  | sle>0.1% | sle>1.0% | sle>3.0% | sle>5.0% | time_spent\n",
    "_Betweenness1                  |          704.85 |          702.07 |     2.78 |     99.60% |  0.0376% |    0.0000% |  2.4893% |        2 |        2 |        0 |        0 |    3.03662\n",
    "_Betweenness2                  |        19116.27 |        19025.63 |    90.64 |     99.52% |  0.0439% |    0.0000% |  3.1837% |        2 |        2 |        1 |        0 |    5.43066\n",
    "_Betweenness3                  |        19299.00 |        19214.00 |    85.00 |     99.56% |  0.0295% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0 |    7.75261\n",
    "_Betweenness4                  |        20938.18 |        20853.18 |    85.00 |     99.59% |  0.0206% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0 |   10.10918\n",
    "Somerville_Bus_Subway_Geometri |          700.29 |          713.14 |   -12.84 |    101.80% |  2.3488% |    2.2893% |  5.0852% |      266 |      232 |       96 |        2 |   14.95809\n",
    "Somerville_Bus_Subway          |          730.95 |          743.95 |   -13.00 |    101.75% |  2.4117% |    2.2917% |  5.1912% |      285 |      251 |      103 |        8 |   30.91729\n",
    "Somerville_Homes_Subway        |       368173.65 |       389383.31 | -21209.66 |    105.45% |     inf% |    7.2526% |     inf% |     2665 |     2631 |     2521 |     2216 |  325.34906\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SIngle add SIngle remove\n",
    "test name                      | madina_flow_sum | Rhino_flow_sum  | sum_diff | sum_smlr_% | sle_mean | sle_median | sle_max  | sle>0.1% | sle>1.0% | sle>3.0% | sle>5.0% | time_spent\n",
    "_Betweenness1                  |          704.85 |          702.07 |     2.78 |     99.60% |  0.0376% |    0.0000% |  2.4893% |        2 |        2 |        0 |        0 |    3.28500\n",
    "_Betweenness2                  |        19116.27 |        19025.63 |    90.64 |     99.52% |  0.0439% |    0.0000% |  3.1837% |        2 |        2 |        1 |        0 |    6.64996\n",
    "_Betweenness3                  |        19299.00 |        19214.00 |    85.00 |     99.56% |  0.0295% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0 |    9.48671\n",
    "_Betweenness4                  |        20938.18 |        20853.18 |    85.00 |     99.59% |  0.0206% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0 |   12.15137\n",
    "Somerville_Bus_Subway_Geometri |          700.29 |          713.14 |   -12.84 |    101.80% |  2.3488% |    2.2893% |  5.0852% |      266 |      232 |       96 |        2 |   17.88752\n",
    "Somerville_Bus_Subway          |          730.95 |          743.95 |   -13.00 |    101.75% |  2.4117% |    2.2917% |  5.1912% |      285 |      251 |      103 |        8 |   36.95279\n",
    "Somerville_Homes_Subway        |       368132.55 |       389383.31 | -21250.76 |    105.46% |     inf% |    7.0262% |     inf% |     2665 |     2619 |     2487 |     2091 |  295.88396\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Classical update light graph\n",
    "test name                      | madina_flow_sum | Rhino_flow_sum  | sum_diff | sum_smlr_% | sle_mean | sle_median | sle_max  | sle>0.1% | sle>1.0% | sle>3.0% | sle>5.0% | time_spent\n",
    "_Betweenness1                  |          704.85 |          702.07 |     2.78 |     99.60% |  0.0376% |    0.0000% |  2.4893% |        2 |        2 |        0 |        0 |    3.66067\n",
    "_Betweenness2                  |        19116.27 |        19025.63 |    90.64 |     99.52% |  0.0439% |    0.0000% |  3.1837% |        2 |        2 |        1 |        0 |    6.61123\n",
    "_Betweenness3                  |        19299.00 |        19214.00 |    85.00 |     99.56% |  0.0295% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0 |    9.72403\n",
    "_Betweenness4                  |        20938.18 |        20853.18 |    85.00 |     99.59% |  0.0206% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0 |   12.61041\n",
    "Somerville_Bus_Subway_Geometri |          700.29 |          713.14 |   -12.84 |    101.80% |  2.3488% |    2.2893% |  5.0852% |      266 |      232 |       96 |        2 |   15.90220\n",
    "Somerville_Bus_Subway          |          730.95 |          743.95 |   -13.00 |    101.75% |  2.4117% |    2.2917% |  5.1912% |      285 |      251 |      103 |        8 |   32.07222\n",
    "Somerville_Homes_Subway        |       368132.55 |       389383.31 | -21250.76 |    105.46% |     inf% |    7.0262% |     inf% |     2665 |     2619 |     2487 |     2091 |  363.67400\n",
    "\n",
    "# paralell_betweenness_exposure (before four fixes...)\n",
    "test name                      | madina_flow_sum | Rhino_flow_sum  | sum_diff | sum_smlr_% | sle_mean | sle_median | sle_max  | sle>0.1% | sle>1.0% | sle>3.0% | sle>5.0% | time_spent\n",
    "_Betweenness1                  |          704.85 |          702.07 |     2.78 |     99.60% |  0.0376% |    0.0000% |  2.4893% |        2 |        2 |        0 |        0 |    4.05741\n",
    "_Betweenness2                  |        19116.27 |        19025.63 |    90.64 |     99.52% |  0.0439% |    0.0000% |  3.1837% |        2 |        2 |        1 |        0 |    7.86811\n",
    "_Betweenness3                  |        19299.00 |        19214.00 |    85.00 |     99.56% |  0.0295% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0 |   11.35870\n",
    "_Betweenness4                  |        20938.18 |        20853.18 |    85.00 |     99.59% |  0.0206% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0 |   15.43956\n",
    "Somerville_Bus_Subway_Geometri |          700.29 |          713.14 |   -12.84 |    101.80% |  2.3488% |    2.2893% |  5.0852% |      266 |      232 |       96 |        2 |   20.17142\n",
    "Somerville_Bus_Subway          |          730.95 |          743.95 |   -13.00 |    101.75% |  2.4117% |    2.2917% |  5.1912% |      285 |      251 |      103 |        8 |   40.65643\n",
    "\n",
    "# paralell_betweenness_exposure (after 5 fixes.....)\n",
    "test name                      | madina_flow_sum | Rhino_flow_sum  | sum_diff | sum_smlr_% | sle_mean | sle_median | sle_max  | sle>0.1% | sle>1.0% | sle>3.0% | sle>5.0% | time_spent\n",
    "_Betweenness1                  |          704.85 |          702.07 |     2.78 |     99.60% |  0.0376% |    0.0000% |  2.4893% |        2 |        2 |        0 |        0 |    3.85227\n",
    "_Betweenness2                  |        19116.27 |        19025.63 |    90.64 |     99.52% |  0.0439% |    0.0000% |  3.1837% |        2 |        2 |        1 |        0 |    7.25895\n",
    "_Betweenness3                  |        19299.00 |        19214.00 |    85.00 |     99.56% |  0.0295% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0 |   10.78336\n",
    "_Betweenness4                  |        20938.18 |        20853.18 |    85.00 |     99.59% |  0.0206% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0 |   14.21737\n",
    "Somerville_Bus_Subway_Geometri |          700.29 |          713.14 |   -12.84 |    101.80% |  2.3488% |    2.2893% |  5.0852% |      266 |      232 |       96 |        2 |   18.31064\n",
    "Somerville_Bus_Subway          |          730.95 |          743.95 |   -13.00 |    101.75% |  2.4117% |    2.2917% |  5.1912% |      285 |      251 |      103 |        8 |   36.99389\n",
    "Somerville_Homes_Subway        |       368132.55 |       389383.31 | -21250.76 |    105.46% |     inf% |    7.0262% |     inf% |     2665 |     2619 |     2487 |     2091 |  429.40144\n",
    "\n",
    "\n",
    "\n",
    "### parallel_betweenness\n",
    "test name                      | madina_flow_sum | Rhino_flow_sum  | sum_diff | sum_smlr_% | sle_mean | sle_median | sle_max  | sle>0.1% | sle>1.0% | sle>3.0% | sle>5.0%\n",
    "_Betweenness1                  |          704.85 |          702.07 |     2.78 |     99.60% |  0.0376% |    0.0000% |  2.4893% |        2 |        2 |        0 |        0\n",
    "_Betweenness2                  |        19116.27 |        19025.63 |    90.64 |     99.52% |  0.0439% |    0.0000% |  3.1837% |        2 |        2 |        1 |        0\n",
    "_Betweenness3                  |        19299.00 |        19214.00 |    85.00 |     99.56% |  0.0295% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0\n",
    "_Betweenness4                  |        20938.18 |        20853.18 |    85.00 |     99.59% |  0.0206% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0\n",
    "Somerville_Bus_Subway_Geometri |          709.44 |          713.14 |    -3.69 |    100.52% |  0.5027% |    0.1954% |  3.9604% |      149 |       62 |        3 |        0\n",
    "Somerville_Bus_Subway          |          740.19 |          743.95 |    -3.76 |    100.51% |  0.5074% |    0.2539% |  3.9604% |      173 |       39 |        3 |        0\n",
    "Somerville_Homes_Subway        |       377922.37 |       389383.31 | -11460.94 |    102.94% |     inf% |    3.8409% |     inf% |     2578 |     2232 |     1591 |      997\n",
    "Somerville_Jobs_Subway         |       146309.13 |       149908.30 | -3599.17 |    102.40% |     inf% |    3.5776% |     inf% |     2078 |     1741 |     1286 |      824\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_new():\n",
    "    betweenness_exposure(\n",
    "        harvard_square,\n",
    "        origins=origin_gdf.iloc[:],\n",
    "        search_radius=test_config.at[test_idx,'Radius'],\n",
    "        detour_ratio=test_config.at[test_idx,'Detour'],\n",
    "        decay=False if test_config.at[test_idx,'Elastic_Weights'] else test_config.at[test_idx,'Decay'],\n",
    "        beta=test_config.at[test_idx,'Beta'],\n",
    "        decay_method=test_config.at[test_idx,'Decay_Mode'],\n",
    "        path_detour_penalty='equal',\n",
    "        closest_destination=test_config.at[test_idx,'Closest_Destination'],\n",
    "        elastic_weight=test_config.at[test_idx,'Elastic_Weights'],\n",
    "        turn_penalty=test_config.at[test_idx,'Turns'],\n",
    "        path_exposure_attribute=None,\n",
    "        return_path_record=False, \n",
    "        destniation_cap=None\n",
    "    )\n",
    "    return\n",
    "\n",
    "\n",
    "def profile_baseline():\n",
    "    one_betweenness_2(\n",
    "        network=harvard_square.network,\n",
    "        search_radius=test_config.at[test_idx,'Radius'],\n",
    "        origins=origin_gdf,\n",
    "        detour_ratio=test_config.at[test_idx,'Detour'],\n",
    "        decay=False if test_config.at[test_idx,'Elastic_Weights'] else test_config.at[test_idx,'Decay'],\n",
    "        beta=test_config.at[test_idx,'Beta'],\n",
    "        decay_method=test_config.at[test_idx,'Decay_Mode'],\n",
    "        path_detour_penalty=\"equal\",  # \"exponent\" | \"power\"\n",
    "        origin_weights=True,\n",
    "        origin_weight_attribute=None,\n",
    "        closest_destination=test_config.at[test_idx,'Closest_Destination'],\n",
    "        destination_weights=True,\n",
    "        destination_weight_attribute=None,\n",
    "        perceived_distance=False,\n",
    "        light_graph=True,\n",
    "        turn_penalty=test_config.at[test_idx,'Turns'],\n",
    "        retained_d_idxs=None,\n",
    "        retained_paths=None,\n",
    "        retained_distances=None,\n",
    "        rertain_expensive_data=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import geopandas as gpd\n",
    "import shapely as shp\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pydeck as pdk\n",
    "\n",
    "print (sys.version)\n",
    "print(f\"{gpd.__version__ = }\")\n",
    "print(f\"{shp.__version__ = }\")\n",
    "print(f\"{nx.__version__ = }\")\n",
    "print(f\"{pd.__version__ = }\")\n",
    "print(f\"{pdk.__version__ = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "def profile_new():\n",
    "\n",
    "    betweenness_exposure(\n",
    "        harvard_square,\n",
    "        origins=origin_gdf,\n",
    "        search_radius=test_config.at[test_idx,'Radius'],\n",
    "        detour_ratio=test_config.at[test_idx,'Detour'],\n",
    "        decay=False if test_config.at[test_idx,'Elastic_Weights'] else test_config.at[test_idx,'Decay'],\n",
    "        beta=test_config.at[test_idx,'Beta'],\n",
    "        decay_method=test_config.at[test_idx,'Decay_Mode'],\n",
    "        path_detour_penalty='equal',\n",
    "        closest_destination=test_config.at[test_idx,'Closest_Destination'],\n",
    "        elastic_weight=test_config.at[test_idx,'Elastic_Weights'],\n",
    "        turn_penalty=test_config.at[test_idx,'Turns'],\n",
    "        path_exposure_attribute=None,\n",
    "        return_path_record=False, \n",
    "        destniation_cap=None\n",
    "    )\n",
    "    return\n",
    "\n",
    "\n",
    "def profile_baseline():\n",
    "    one_betweenness_2(\n",
    "        network=harvard_square.network,\n",
    "        search_radius=test_config.at[test_idx,'Radius'],\n",
    "        origins=origin_gdf,\n",
    "        detour_ratio=test_config.at[test_idx,'Detour'],\n",
    "        decay=False if test_config.at[test_idx,'Elastic_Weights'] else test_config.at[test_idx,'Decay'],\n",
    "        beta=test_config.at[test_idx,'Beta'],\n",
    "        decay_method=test_config.at[test_idx,'Decay_Mode'],\n",
    "        path_detour_penalty=\"equal\",  # \"exponent\" | \"power\"\n",
    "        origin_weights=True,\n",
    "        origin_weight_attribute=None,\n",
    "        closest_destination=test_config.at[test_idx,'Closest_Destination'],\n",
    "        destination_weights=True,\n",
    "        destination_weight_attribute=None,\n",
    "        perceived_distance=False,\n",
    "        light_graph=True,\n",
    "        turn_penalty=test_config.at[test_idx,'Turns'],\n",
    "        retained_d_idxs=None,\n",
    "        retained_paths=None,\n",
    "        retained_distances=None,\n",
    "        rertain_expensive_data=False\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun profile_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun profile_new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets = harvard_square.layers['streets'].gdf\n",
    "path_record = return_dict['path_record']\n",
    "\n",
    "harvard_square.create_map(\n",
    "    [\n",
    "        {'gdf': streets, 'color': [100, 100, 100], 'opacity': 0.1},\n",
    "        {'gdf': path_record[path_record['origin_id'] == 7796], 'color_by_attribute': 'path_id', 'color_method': 'categorical', 'opacity': 0.1, 'text': 'path_betweenness'},\n",
    "        {'gdf': origin_layer, 'color': [100, 0, 255]},\n",
    "        {'gdf': origin_nodes, 'color': [100, 0, 255]},\n",
    "        {'gdf': origin_joined[['geometry']], 'color': [100, 0, 255]},\n",
    "        {'gdf': destination_layer, 'color': [255, 0, 100]},\n",
    "        {'gdf': destination_joined[['geometry']], 'color': [255, 0, 100]},\n",
    "        {'gdf': destination_nodes, 'color': [255, 0, 100]},\n",
    "    ],\n",
    "    #save_as=\"Test Cases\\\\\" + test_case + '\\\\'  + test_name + \"._difference_map.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Light Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_light_graph(self, graph: nx.Graph, add_nodes: list = [], remove_nodes: list = []):\n",
    "    \"\"\"\n",
    "    Updates the given graph object by adding nodes to and removing nodes from it.\n",
    "\n",
    "    Args:\n",
    "        graph: The given networkx Graph object to be edited\n",
    "        add_nodes: a list of nodes to be added\n",
    "        remove_nodes: a list of nodes to be removed\n",
    "\n",
    "    Returns:\n",
    "        none\n",
    "    \"\"\"\n",
    "    \n",
    "    if \"added_nodes\" not in graph.graph:\n",
    "        graph.graph[\"added_nodes\"] = []\n",
    "\n",
    "    # Add nodes\n",
    "    if len(add_nodes) > 0:\n",
    "\n",
    "        existing_nodes = graph.graph[\"added_nodes\"]\n",
    "        for node_idx in add_nodes:\n",
    "            if node_idx in existing_nodes:\n",
    "                print(f'node ({node_idx}) is already added...{add_nodes = }\\t{existing_nodes = }')\n",
    "                print(graph)\n",
    "                add_nodes.remove(node_idx)\n",
    "            else:\n",
    "                graph.graph[\"added_nodes\"].append(node_idx)\n",
    "\n",
    "        edge_nodes = {}\n",
    "        for key, value in self.nodes.loc[graph.graph[\"added_nodes\"]].groupby(\"nearest_edge_id\"):\n",
    "            edge_nodes[int(key)] = list(value.index)\n",
    "            \n",
    "        for edge_id in edge_nodes:\n",
    "            neighbors = edge_nodes[edge_id]\n",
    "            insert_neighbors = set(add_nodes).intersection(set(neighbors))\n",
    "            existing_neighbors = set(neighbors) - insert_neighbors\n",
    "\n",
    "            if len(insert_neighbors) == 0:\n",
    "                continue\n",
    "                #self.nodes.at[node_idx, \"\"]\n",
    "            if len(neighbors) == 1:\n",
    "                node_idx = neighbors[0]\n",
    "                graph.add_edge(\n",
    "                    int(self.nodes.at[node_idx, \"edge_end_node\"]),\n",
    "                    int(node_idx),\n",
    "                    weight=max(self.nodes.at[node_idx, \"weight_to_end\"], 0),\n",
    "                    id=edge_id\n",
    "                )\n",
    "                graph.add_edge(\n",
    "                    int(node_idx),\n",
    "                    int(self.nodes.at[node_idx, \"edge_start_node\"]),\n",
    "                    weight=max(self.nodes.at[node_idx, 'weight_to_start'], 0),\n",
    "                    id=edge_id\n",
    "                )\n",
    "                graph.remove_edge(self.nodes.at[node_idx, \"edge_end_node\"], int(self.nodes.at[node_idx, \"edge_start_node\"]))\n",
    "                \n",
    "            else:\n",
    "                # start a chain addition of neighbors, starting from the 'left',\n",
    "                # so, need to sort based on distance from left\n",
    "                segment_weight = self.edges.at[edge_id, \"weight\"]\n",
    "\n",
    "                chain_start = self.nodes.at[neighbors[0], \"edge_end_node\"]\n",
    "                chain_end = self.nodes.at[neighbors[0], \"edge_start_node\"]\n",
    "\n",
    "                chain_distances = [self.nodes.at[node, \"weight_to_end\"] for node in neighbors]\n",
    "\n",
    "                if len(existing_neighbors) == 0:  # if there are no existing neighbors, remove the original edge\n",
    "                    graph.remove_edge(int(chain_start), int(chain_end))\n",
    "                else:  # if there are existing neighbors, remove them. This would also remove their associated edges\n",
    "                    for node in existing_neighbors:\n",
    "                        graph.remove_node(node)\n",
    "\n",
    "                chain_nodes = [chain_start] + neighbors + [chain_end]\n",
    "                chain_distances = [0] + chain_distances + [segment_weight]\n",
    "\n",
    "                chain_nodes = np.array(chain_nodes)\n",
    "                chain_distances = np.array(chain_distances)\n",
    "                sorting_index = np.argsort(chain_distances)\n",
    "\n",
    "                # np arrays allows calling them by a list index\n",
    "                chain_nodes = chain_nodes[sorting_index]\n",
    "                chain_distances = chain_distances[sorting_index]\n",
    "                # print(f\"{chain_nodes = }\\t{chain_distances}\")\n",
    "                accumilated_weight = 0\n",
    "                for seq in range(len(chain_nodes) - 1):\n",
    "                    graph.add_edge(\n",
    "                        int(chain_nodes[seq]),\n",
    "                        int(chain_nodes[seq + 1]),\n",
    "                        # TODO: change this to either defaults to distance or a specified column for a weight..\n",
    "                        weight=max(chain_distances[seq + 1] - chain_distances[seq], 0),\n",
    "                        ##avoiding small negative numbers due to numerical error when two nodes are superimposed.\n",
    "                        id=edge_id\n",
    "                    )\n",
    "                    accumilated_weight += (chain_distances[seq + 1] - chain_distances[seq])\n",
    "    \n",
    "    # Removing nodes\n",
    "    for node_idx in remove_nodes:\n",
    "        node_idx = int(node_idx)\n",
    "        if node_idx not in graph.nodes:\n",
    "            print(f\"attempting to remove node {node_idx} that's not in graph {str(graph)}\")\n",
    "            continue\n",
    "\n",
    "        if len(graph.adj[node_idx]) != 2:\n",
    "            print(f\"attempting to remove a node {node_idx = } that's not degree 2, adjacent to: {graph.adj[node_idx]}\")\n",
    "            continue\n",
    "\n",
    "        neighbors = list(graph.adj[node_idx])\n",
    "\n",
    "        start = int(neighbors[0])\n",
    "        end = int(neighbors[1])\n",
    "        weight = graph.adj[node_idx][start][\"weight\"] \\\n",
    "                + graph.adj[node_idx][end][\"weight\"]\n",
    "\n",
    "        original_edge_id = self.nodes.at[node_idx, \"nearest_edge_id\"]\n",
    "\n",
    "        # remove node after we got the attributes we needed..\n",
    "        graph.remove_node(node_idx)\n",
    "        graph.graph[\"added_nodes\"].remove(node_idx)\n",
    "        graph.add_edge(\n",
    "            start,\n",
    "            end,\n",
    "            weight=weight,\n",
    "            id=original_edge_id\n",
    "        )\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for origin_idx in origin_gdf.index:\n",
    "    start = time.time()\n",
    "    update_light_graph(\n",
    "        harvard_square.network,\n",
    "        harvard_square.network.d_graph,\n",
    "        add_nodes=[origin_idx],\n",
    "        #remove_nodes=[origin_idx]\n",
    "    )\n",
    "\n",
    "    update_light_graph(\n",
    "        harvard_square.network,\n",
    "        harvard_square.network.d_graph,\n",
    "        #add_nodes=[origin_idx],\n",
    "        remove_nodes=[origin_idx]\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COnstructing the Manhattan Case..\n",
    "## Don't forget to check for node insertion, set  return_all=False in the spatial index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "buildings_file = r\"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\Test Cases\\Manhattan\\Home_PT_6538.geojson\"\n",
    "subway_file = r\"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\Test Cases\\Manhattan\\Metro_PT_6538.geojson\"\n",
    "network_file = r\"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\Test Cases\\Manhattan\\network_clipped_dupremovedAS.geojson\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from madina.zonal.zonal import Zonal\n",
    "\n",
    "\n",
    "harvard_square = Zonal()\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t imports done, object created\")\n",
    "start = time.time()\n",
    "\n",
    "#harvard_square.load_layer(\n",
    "#    layer_name='streets',\n",
    "#    file_path=network_file\n",
    "#    )\n",
    "\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t street data loaded\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.load_layer(\n",
    "    layer_name='buildings',\n",
    "    file_path=buildings_file\n",
    "    )\n",
    "\n",
    "buildings_gdf = harvard_square.layers['buildings'].gdf\n",
    "harvard_square.layers['buildings'].gdf = buildings_gdf[~buildings_gdf['FID'].isin([27967, 9140, 3974])]\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t building data loaded\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.load_layer(\n",
    "    layer_name='subway',\n",
    "    file_path=subway_file\n",
    "    )\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t subway data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "harvard_square.create_street_network(\n",
    "    source_layer='streets', \n",
    "    discard_redundant_edges=False, \n",
    "    node_snapping_tolerance=0\n",
    ")\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t street network created\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.insert_node(\n",
    "    layer_name='buildings', \n",
    "    label='origin', \n",
    "    weight_attribute='TotalPop'\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t origins insertes\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.insert_node(\n",
    "    layer_name='subway', \n",
    "    label='destination', \n",
    "    weight_attribute='line_ent_st'\n",
    ")\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t destinations insertes\")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.create_graph(light_graph=True, d_graph=True)\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t graph created insertes\")\n",
    "start = time.time()\n",
    "\n",
    "return_dict = parallel_betweenness(\n",
    "    harvard_square.network,\n",
    "    search_radius=800,\n",
    "    detour_ratio=1.15,\n",
    "    decay=False,\n",
    "    decay_method='exponent',  # \"power\", \"exponent\"\n",
    "    beta=0.004,\n",
    "    path_detour_penalty=\"equal\",  # \"power\", \"exponent\", \"equal\"\n",
    "    origin_weights=True,\n",
    "    closest_destination=False,\n",
    "    destination_weights=True, \n",
    "    # perceived_distance=False,\n",
    "    num_cores=8,\n",
    "    light_graph=True,\n",
    "    turn_penalty=False,\n",
    ")\n",
    "simulated_sum_of_flow = return_dict['edge_gdf']['betweenness'].sum()\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t Betweenness estimated\")\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_results = harvard_square.layers['streets'].gdf.join(harvard_square.network.edges[['parent_street_id', 'betweenness']].set_index('parent_street_id'))\n",
    "joined_results[['__GUID', 'betweenness', 'geometry']].to_csv('2023-07-07 manhattan betweenness flow test.csv')\n",
    "joined_results[['__GUID', 'betweenness', 'geometry']].to_file('2023-07-07 manhattan betweenness flow test.geoJSON', driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard_square.create_map(\n",
    "    [\n",
    "        #{\n",
    "            #'layer': 'streets',\n",
    "            #'color': [125, 125, 125],\n",
    "        #},\n",
    "        {\n",
    "            'gdf': harvard_square.layers['homes'].gdf,\n",
    "            'color_by_attribute': 'line_ent_st',\n",
    "            \"color_method\": 'gradient'\n",
    "        }\n",
    "    ],\n",
    "    save_as=\"map.html\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "madina_env_latest_updates",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
