{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tuesday 2023-07-11 action items:\n",
    "* ~~Debug Beirut, see where the source of decreased knn weight~~\n",
    "* ~~re run an accessibility simulation, as well as a betweenness flow simulation for beirut~~\n",
    "* ~~Once simulation accessibility results done, send to danie~~ addd flows to folder and send another email\n",
    "* ~~create visuak maos for scenarios showing increase in reach.. Add to google slides~~\n",
    "* ~~Homes to > transit - parks - amenities - schools~~\n",
    "* ~~geometric - no turns, no elastic weight, no decay ....~~\n",
    "* ~~la_extended16_manualFixed~~\n",
    "* ~~Run Somerville~~\n",
    "* ~~RUn LA~~\n",
    "* ~~look at Andres' email about brooklyn , run a simulation using the old code in the workstation~~\n",
    "* ~~look at Lui's email, see if I need to do anything about it~~\n",
    "* ~~create OD pairing table for LA, send to Andres~~\n",
    "* ~~RUN A FKOW MODEL FOR HOMES TO MOSKS, USING THE SAME PARAMETERS, WITH KNN , FOR ALL SCENARIOS, SEND TO DANIEL~~\n",
    "* ~~CLip lui's dataset into the area that daniel chose, share with andres~~\n",
    "*~~ FOR the clip, use this file \"C:\\Users\\abdul\\Dropbox (MIT)\\115_NYCWalks\\03_Data\\02_Edited\\NYC_Sidewalk_Edit_July\\BK_clipping.geojson\"~~\n",
    "* ~~make sure the new node insertion code works and matches well~~\n",
    "\n",
    "## effecient node edge creation\n",
    "* ~~Make sure the functionality to construct nodes-edges works for a safey buffer~~\n",
    "* ~~Isolate the node insertion code into script~~\n",
    "* ~~commit script to my branch~~\n",
    "* ~~email orion about it, mention how easy to integrate.~~\n",
    "* ~~look into issue with redundant edges~~\n",
    "\n",
    "## Testing\n",
    "* ~~structure tests so its a one csv: first n rows are settings, next m rows are edges flows, with first column being same segment ids from network file~~\n",
    "* ~~standard test case is four files: network.jeojson - origins.geojson - destination.jeojson - testflows.csv~~\n",
    "* ~~function reads a csv into list of dicts: test settings and series of output.~~\n",
    "* ~~build Harvard Square testflows.csv~~\n",
    "~~* make sure this config runs on Harvard Square~~\n",
    "\n",
    "\n",
    "\n",
    "# TODO\n",
    "## Functionality List 1\n",
    "* add visualization functionality to new version of Madina so testing is easier\n",
    "* in network, add  connector segments as a separate layer (Maybe not very needed if origins and destinations visually are snapoped...)\n",
    "* Commit visualization updates to github\n",
    "\n",
    "\n",
    "## Testing\n",
    "* digitize three test cases for manhattan, visualize OD snapping, show where differences (if any) come from between Rhino  and Python in the NYC case.\n",
    "* Put maps into slides\n",
    "* CHeck somerville, to see if we delete the same number of edges, with and without the effecient code \"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\Cities\\Somerville\\Data\\Somerville_network_1000m_buffer.geojson\"\n",
    "\n",
    "## Functionality List 2\n",
    "* build the betweenness simulation workflow, combine with accissibility\n",
    "* include Logger as a class, and zonal would have an instance of that class. Logger handles event documentation, and captures output.\n",
    "* for each origin, have an origin record, showing its knn weight, reach, gravity towards each of its destination, header rows showing settings and parameters\n",
    "* for the network, columns of od flows, headed  by settings and parameters rows that explicitly detail units, weight type, calibration status, .. any relevant data, settings and parameters..\n",
    "* set up a slideshow/diagram to show the relationship between different compoonents of the library.\n",
    "* Set up a notebook to go over estimating flows from one origin to one destination, Document all relevant settings\n",
    "* show how this generalizes to the pairings.csv, iterating over moodel settings, iterating over scenarios\n",
    "* Make a finalized github commit including testing script, betweenness flow notebook. \n",
    "* Deploy to workstation.\n",
    "\n",
    "\n",
    "## Functionality List 3\n",
    "* given the simulation results, and the censor locations, find a way to calibrate counts and generate beta coeffecients (Beirut, SOmerville, NYC)\n",
    "* enable Jupyter notebook server access from workstation.\n",
    "* Figure out a way to do the \"Cities\" dropbox folder, people can add data through dropbox, run simulations through Jupyter Server, get results through Dropbox\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Running Cases\n",
    "* Send output of LA to Nico when done\n",
    "* send Brooklyn output when done\n",
    "* be prepared to run entire NYC simulation, be mindful that daniel already sent more network parts\n",
    "\n",
    "\n",
    "## Map Cleaning\n",
    "https://www.dropbox.com/scl/fo/d6ugcyt3aanfb3yk4n9va/h?dl=0&rlkey=1c2giilvzoyygcxrlry7fscr9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm import tqdm\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "from madina.zonal.zonal import Zonal\n",
    "from madina.una.betweenness import parallel_betweenness\n",
    "from madina.una.elastic import get_elastic_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_case in os.listdir(\"Test Cases\"):\n",
    "    # TODO: Check OS compatibility, ensure this is compatible with Unix systems..\n",
    "    test_case_folder = \"Test Cases\" + \"\\\\\" + test_case + \"\\\\\"\n",
    "    test_config = pd.read_csv(test_case_folder + \"test_configs.csv\")\n",
    "    test_flows =  pd.read_csv(test_case_folder + \"test_flows.csv\")\n",
    "\n",
    "    harvard_square = Zonal(projected_crs='EPSG:3857')\n",
    "\n",
    "    harvard_square.load_layer(\n",
    "        layer_name='streets',\n",
    "        file_path=  test_case_folder + test_config.at[0, 'Network_File']\n",
    "        )\n",
    "\n",
    "    harvard_square.load_layer(\n",
    "        layer_name=test_config.at[0, 'Origin_Name'],\n",
    "        file_path= test_case_folder + test_config.at[0, 'Origin_File']\n",
    "        )\n",
    "\n",
    "    harvard_square.load_layer(\n",
    "        layer_name=test_config.at[0, 'Destination_Name'],\n",
    "        file_path= test_case_folder + test_config.at[0, 'Destination_File']\n",
    "        )\n",
    "    \n",
    "    harvard_square.create_street_network(\n",
    "        source_layer='streets', \n",
    "        discard_redundant_edges=True,\n",
    "        node_snapping_tolerance=1.0\n",
    "    )\n",
    "\n",
    "    harvard_square.insert_node(\n",
    "        layer_name=test_config.at[0, 'Origin_Name'], \n",
    "        label='origin', \n",
    "        weight_attribute=test_config.at[3, 'Origin_Weight']\n",
    "    )\n",
    "\n",
    "    harvard_square.insert_node(\n",
    "        layer_name=test_config.at[0, 'Destination_Name'], \n",
    "        label='destination', \n",
    "        weight_attribute=test_config.at[3, 'Destination_Weight']\n",
    "    )\n",
    "\n",
    "    harvard_square.create_graph(light_graph=True, d_graph=True)\n",
    "\n",
    "    node_gdf = harvard_square.network.nodes\n",
    "    origin_gdf = node_gdf[node_gdf['type'] == 'origin']\n",
    "\n",
    "    harvard_square.network.nodes[\"original_weight\"] = harvard_square.network.nodes[\"weight\"]\n",
    "\n",
    "\n",
    "    # [\"original_weight\", \"elastic_weight\", \"knn_weight\"]\n",
    "\n",
    "    for test_idx in test_config.index:\n",
    "        harvard_square.network.turn_penalty_amount = test_config.at[test_idx, 'Turn penalty']\n",
    "        harvard_square.network.turn_threshold_degree = test_config.at[test_idx, 'Turn threshold']\n",
    "\n",
    "        if test_config.at[test_idx, 'Elastic_weights']:\n",
    "            harvard_square.network.nodes[\"weight\"] = harvard_square.network.nodes[\"original_weight\"]\n",
    "            get_elastic_weight(\n",
    "                harvard_square.network,\n",
    "                search_radius=test_config.at[test_idx, 'Radius'],\n",
    "                detour_ratio=test_config.at[test_idx, 'Detour'],\n",
    "                beta=test_config.at[test_idx, ' Beta '],\n",
    "                decay=True, #test_config.at[test_idx, 'Decay'],\n",
    "                #turn_penalty=test_config.at[test_idx, 'Turns'],\n",
    "                turn_penalty=False,\n",
    "            )\n",
    "            for o_idx in origin_gdf.index:\n",
    "                harvard_square.network.nodes.at[o_idx, 'weight'] =  harvard_square.network.nodes.at[o_idx, 'elastic_weight']\n",
    "\n",
    "\n",
    "        return_dict = parallel_betweenness(\n",
    "            harvard_square.network,\n",
    "            search_radius=test_config.at[test_idx, 'Radius'],\n",
    "            detour_ratio=test_config.at[test_idx, 'Detour'],\n",
    "            decay=test_config.at[test_idx, 'Decay'], #if test['Elastic weights'] else True,\n",
    "            decay_method=test_config.at[test_idx, 'Decay_Mode'],  # \"power\", \"exponent\"\n",
    "            beta=test_config.at[test_idx, ' Beta '],\n",
    "            path_detour_penalty=\"equal\",  # \"power\", \"exponent\", \"equal\"\n",
    "            origin_weights=False if type(test_config.at[test_idx, 'Origin_Weight']) != str else True,\n",
    "            closest_destination=test_config.at[test_idx, 'Closest_destination'],\n",
    "            destination_weights=False if type(test_config.at[test_idx, 'Destination_Weight']) != str  else True,    #or (test['Elastic weights'])\n",
    "            # perceived_distance=False,\n",
    "            num_cores=2,\n",
    "            light_graph=True,\n",
    "            turn_penalty=test_config.at[test_idx, 'Turns'],\n",
    "        )\n",
    "        simulated_sum_of_flow = return_dict['edge_gdf']['betweenness'].sum()\n",
    "        test_flow = test_flows[test_config.at[test_idx, 'test_name']].sum()\n",
    "\n",
    "        print (test_config.loc[test_idx])\n",
    "        print (f\"{test_config.at[test_idx, 'test_name']}\\t\\t{simulated_sum_of_flow = }\\t test flow = { test_flow }\\t difference = {simulated_sum_of_flow - test_flow}\\t similarity {1-(simulated_sum_of_flow - test_flow)/ test_flow:.2%}\")\n",
    "    print (\"DOne Case...\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COnstructing the Manhattan Case..\n",
    "## Don't forget to check for node insertion, set  return_all=False in the spatial index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "buildings_file = r\"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\Test Cases\\Manhattan\\Home_PT_6538.geojson\"\n",
    "subway_file = r\"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\Test Cases\\Manhattan\\Metro_PT_6538.geojson\"\n",
    "network_file = r\"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\Test Cases\\Manhattan\\network_clipped_dupremovedAS.geojson\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from madina.zonal.zonal import Zonal\n",
    "\n",
    "\n",
    "harvard_square = Zonal()\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t imports done, object created\")\n",
    "start = time.time()\n",
    "\n",
    "#harvard_square.load_layer(\n",
    "#    layer_name='streets',\n",
    "#    file_path=network_file\n",
    "#    )\n",
    "\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t street data loaded\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.load_layer(\n",
    "    layer_name='buildings',\n",
    "    file_path=buildings_file\n",
    "    )\n",
    "\n",
    "buildings_gdf = harvard_square.layers['buildings'].gdf\n",
    "harvard_square.layers['buildings'].gdf = buildings_gdf[~buildings_gdf['FID'].isin([27967, 9140, 3974])]\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t building data loaded\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.load_layer(\n",
    "    layer_name='subway',\n",
    "    file_path=subway_file\n",
    "    )\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t subway data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "harvard_square.create_street_network(\n",
    "    source_layer='streets', \n",
    "    discard_redundant_edges=False, \n",
    "    node_snapping_tolerance=0\n",
    ")\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t street network created\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.insert_node(\n",
    "    layer_name='buildings', \n",
    "    label='origin', \n",
    "    weight_attribute='TotalPop'\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t origins insertes\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.insert_node(\n",
    "    layer_name='subway', \n",
    "    label='destination', \n",
    "    weight_attribute='line_ent_st'\n",
    ")\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t destinations insertes\")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.create_graph(light_graph=True, d_graph=True)\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t graph created insertes\")\n",
    "start = time.time()\n",
    "\n",
    "return_dict = parallel_betweenness(\n",
    "    harvard_square.network,\n",
    "    search_radius=800,\n",
    "    detour_ratio=1.15,\n",
    "    decay=False,\n",
    "    decay_method='exponent',  # \"power\", \"exponent\"\n",
    "    beta=0.004,\n",
    "    path_detour_penalty=\"equal\",  # \"power\", \"exponent\", \"equal\"\n",
    "    origin_weights=True,\n",
    "    closest_destination=False,\n",
    "    destination_weights=True, \n",
    "    # perceived_distance=False,\n",
    "    num_cores=8,\n",
    "    light_graph=True,\n",
    "    turn_penalty=False,\n",
    ")\n",
    "simulated_sum_of_flow = return_dict['edge_gdf']['betweenness'].sum()\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t Betweenness estimated\")\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_results = harvard_square.layers['streets'].gdf.join(harvard_square.network.edges[['parent_street_id', 'betweenness']].set_index('parent_street_id'))\n",
    "joined_results[['__GUID', 'betweenness', 'geometry']].to_csv('2023-07-07 manhattan betweenness flow test.csv')\n",
    "joined_results[['__GUID', 'betweenness', 'geometry']].to_file('2023-07-07 manhattan betweenness flow test.geoJSON', driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard_square.create_map(\n",
    "    [\n",
    "        #{\n",
    "            #'layer': 'streets',\n",
    "            #'color': [125, 125, 125],\n",
    "        #},\n",
    "        {\n",
    "            'gdf': harvard_square.layers['subway'].gdf,\n",
    "            'color_by_attribute': 'line_ent_st',\n",
    "            \"color_method\": 'gradient'\n",
    "        }\n",
    "    ],\n",
    "    save_as=\"map.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to instal pydeck jupyter nb extension\n",
    "\n",
    "`jupyter nbextension install --sys-prefix --symlink --overwrite --py pydeck`\n",
    "\n",
    "`jupyter nbextension enable --sys-prefix --py pydeck`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "madina_env_latest_updates",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
