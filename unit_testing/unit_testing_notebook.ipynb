{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023-07-07\n",
    "## Meeting outcomes\n",
    "* CLip lui's dataset into the area that daniel chose, share with andres\n",
    "* Wait for Daniel to send network, for me to run a betweenness flow simulation\n",
    "* Use the Brooklyn case as a test case for betweenness variation between rhino and pytho\n",
    "\n",
    "\n",
    "## Proprity for me after discussion with orion:\n",
    "* make sure the new node insertion code works and matches well]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps forward:\n",
    "\n",
    "* SEND UPDATE ABOUT NYC WHEN TEST CASE IS DONE\n",
    "\n",
    "## effecient node edge creation\n",
    "* ~~Make sure the functionality to construct nodes-edges works for a safey buffer~~\n",
    "* ~~Isolate the node insertion code into script~~\n",
    "* ~~commit script to my branch~~\n",
    "* ~~email orion about it, mention how easy to integrate.~~\n",
    "* look into issue with redundant edges\n",
    "\n",
    "## Testing\n",
    "* ~~structure tests so its a one csv: first n rows are settings, next m rows are edges flows, with first column being same segment ids from network file~~\n",
    "* ~~standard test case is four files: network.jeojson - origins.geojson - destination.jeojson - testflows.csv~~\n",
    "* ~~function reads a csv into list of dicts: test settings and series of output.~~\n",
    "* ~~build Harvard Square testflows.csv~~\n",
    "~~* make sure this config runs on Harvard Square~~\n",
    "* build Manhattan testflow.csv\n",
    "\n",
    "## Documentation\n",
    "* set up a slideshow/diagram to show the relationship between different compoonents of the library.\n",
    "* Set up a notebook to go over estimating flows from one origin to one destination.\n",
    "* Document all relevant settings\n",
    "* show how this generalizes to the pairings.csv\n",
    "* show how this generalizes to iterating over moodel settings\n",
    "* show how this generalizes to iterating over scenarios\n",
    "\n",
    "## input structure\n",
    "maybe some parameters are global to zonal. Updated only when needed, instead of passed as inputs?\n",
    "\n",
    "## output structure\n",
    "* include Logger as a class, and zonal would have an instance of that class. Logger handles event documentation, and captures output.\n",
    "* think clearly about the above usecases and structure output accordingly\n",
    "* for each origin, have an origin record, showing its knn weight, reach, gravity towards each of its destination, header rows showing settings and parameters\n",
    "* for the network, columns of od flows, headed  by settings and parameters rows that explicitly detail units, weight type, calibration status, .. any relevant data, settings and parameters..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "from madina.zonal.zonal import Zonal\n",
    "from madina.una.betweenness import parallel_betweenness\n",
    "from madina.una.elastic import get_elastic_weight\n",
    "\n",
    "for test_case in os.listdir(\"Test Cases\"):\n",
    "    # TODO: Check OS compatibility, ensure this is compatible with Unix systems..\n",
    "    test_case_folder = \"Test Cases\" + \"\\\\\" + test_case + \"\\\\\"\n",
    "    test_config = pd.read_csv(test_case_folder + \"test_configs.csv\")\n",
    "    test_flows =  pd.read_csv(test_case_folder + \"test_flows.csv\")\n",
    "\n",
    "    harvard_square = Zonal(projected_crs='EPSG:3857')\n",
    "\n",
    "    harvard_square.load_layer(\n",
    "        layer_name='streets',\n",
    "        file_path=  test_case_folder + test_config.at[0, 'Network_File']\n",
    "        )\n",
    "\n",
    "    harvard_square.load_layer(\n",
    "        layer_name=test_config.at[0, 'Origin_Name'],\n",
    "        file_path= test_case_folder + test_config.at[0, 'Origin_File']\n",
    "        )\n",
    "\n",
    "    harvard_square.load_layer(\n",
    "        layer_name=test_config.at[0, 'Destination_Name'],\n",
    "        file_path= test_case_folder + test_config.at[0, 'Destination_File']\n",
    "        )\n",
    "    \n",
    "    harvard_square.create_street_network(\n",
    "        source_layer='streets', \n",
    "        discard_redundant_edges=True,\n",
    "        node_snapping_tolerance=1.0\n",
    "    )\n",
    "\n",
    "    harvard_square.insert_node(\n",
    "        layer_name=test_config.at[0, 'Origin_Name'], \n",
    "        label='origin', \n",
    "        weight_attribute=test_config.at[3, 'Origin_Weight']\n",
    "    )\n",
    "\n",
    "    harvard_square.insert_node(\n",
    "        layer_name=test_config.at[0, 'Destination_Name'], \n",
    "        label='destination', \n",
    "        weight_attribute=test_config.at[3, 'Destination_Weight']\n",
    "    )\n",
    "\n",
    "    harvard_square.create_graph(light_graph=True, d_graph=True)\n",
    "\n",
    "    node_gdf = harvard_square.network.nodes\n",
    "    origin_gdf = node_gdf[node_gdf['type'] == 'origin']\n",
    "\n",
    "    harvard_square.network.nodes[\"original_weight\"] = harvard_square.network.nodes[\"weight\"]\n",
    "\n",
    "\n",
    "    # [\"original_weight\", \"elastic_weight\", \"knn_weight\"]\n",
    "\n",
    "    for test_idx in test_config.index:\n",
    "        harvard_square.network.turn_penalty_amount = test_config.at[test_idx, 'Turn penalty']\n",
    "        harvard_square.network.turn_threshold_degree = test_config.at[test_idx, 'Turn threshold']\n",
    "\n",
    "        if test_config.at[test_idx, 'Elastic_weights']:\n",
    "            harvard_square.network.nodes[\"weight\"] = harvard_square.network.nodes[\"original_weight\"]\n",
    "            get_elastic_weight(\n",
    "                harvard_square.network,\n",
    "                search_radius=test_config.at[test_idx, 'Radius'],\n",
    "                detour_ratio=test_config.at[test_idx, 'Detour'],\n",
    "                beta=test_config.at[test_idx, ' Beta '],\n",
    "                decay=True, #test_config.at[test_idx, 'Decay'],\n",
    "                #turn_penalty=test_config.at[test_idx, 'Turns'],\n",
    "                turn_penalty=False,\n",
    "            )\n",
    "            for o_idx in origin_gdf.index:\n",
    "                harvard_square.network.nodes.at[o_idx, 'weight'] =  harvard_square.network.nodes.at[o_idx, 'elastic_weight']\n",
    "\n",
    "\n",
    "        return_dict = parallel_betweenness(\n",
    "            harvard_square.network,\n",
    "            search_radius=test_config.at[test_idx, 'Radius'],\n",
    "            detour_ratio=test_config.at[test_idx, 'Detour'],\n",
    "            decay=test_config.at[test_idx, 'Decay'], #if test['Elastic weights'] else True,\n",
    "            decay_method=test_config.at[test_idx, 'Decay_Mode'],  # \"power\", \"exponent\"\n",
    "            beta=test_config.at[test_idx, ' Beta '],\n",
    "            path_detour_penalty=\"equal\",  # \"power\", \"exponent\", \"equal\"\n",
    "            origin_weights=False if type(test_config.at[test_idx, 'Origin_Weight']) != str else True,\n",
    "            closest_destination=test_config.at[test_idx, 'Closest_destination'],\n",
    "            destination_weights=False if type(test_config.at[test_idx, 'Destination_Weight']) != str  else True,    #or (test['Elastic weights'])\n",
    "            # perceived_distance=False,\n",
    "            num_cores=2,\n",
    "            light_graph=True,\n",
    "            turn_penalty=test_config.at[test_idx, 'Turns'],\n",
    "        )\n",
    "        simulated_sum_of_flow = return_dict['edge_gdf']['betweenness'].sum()\n",
    "        test_flow = test_flows[test_config.at[test_idx, 'test_name']].sum()\n",
    "\n",
    "        print (test_config.loc[test_idx])\n",
    "        print (f\"{test_config.at[test_idx, 'test_name']}\\t\\t{simulated_sum_of_flow = }\\t test flow = { test_flow }\\t difference = {simulated_sum_of_flow - test_flow}\\t similarity {1-(simulated_sum_of_flow - test_flow)/ test_flow:.2%}\")\n",
    "    print (\"DOne Case...\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "buildings_file = r\"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\Test Cases\\Manhattan\\Home_PT_6538.geojson\"\n",
    "subway_file = r\"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\Test Cases\\Manhattan\\Metro_PT_6538.geojson\"\n",
    "network_file = r\"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\Test Cases\\Manhattan\\network_clipped_dupremovedAS.geojson\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from madina.zonal.zonal import Zonal\n",
    "\n",
    "\n",
    "harvard_square = Zonal(projected_crs='EPSG:6538')\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t imports done, object created\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.load_layer(\n",
    "    layer_name='streets',\n",
    "    file_path=network_file\n",
    "    )\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t street data loaded\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.load_layer(\n",
    "    layer_name='buildings',\n",
    "    file_path=buildings_file\n",
    "    )\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t building data loaded\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.load_layer(\n",
    "    layer_name='subway',\n",
    "    file_path=subway_file\n",
    "    )\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t subway data loaded\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.create_street_network(\n",
    "    source_layer='streets', \n",
    "    discard_redundant_edges=False, \n",
    "    node_snapping_tolerance=1.0\n",
    ")\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t street network created\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.insert_node(\n",
    "    layer_name='buildings', \n",
    "    label='origin', \n",
    "    weight_attribute='TotalPop'\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t origins insertes\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.insert_node(\n",
    "    layer_name='subway', \n",
    "    label='destination', \n",
    "    weight_attribute='line_ent_st'\n",
    ")\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t destinations insertes\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.create_graph(light_graph=True, d_graph=True)\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t graph created insertes\")\n",
    "start = time.time()\n",
    "\n",
    "node_gdf = harvard_square.network.nodes\n",
    "origin_gdf = node_gdf[node_gdf['type'] == 'origin']\n",
    "\n",
    "harvard_square.network.nodes[\"original_weight\"] = harvard_square.network.nodes[\"weight\"]\n",
    "# [\"original_weight\", \"elastic_weight\", \"knn_weight\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "return_dict = parallel_betweenness(\n",
    "    harvard_square.network,\n",
    "    search_radius=800,\n",
    "    detour_ratio=1.15,\n",
    "    decay=False,\n",
    "    decay_method='exponent',  # \"power\", \"exponent\"\n",
    "    beta=0.004,\n",
    "    path_detour_penalty=\"equal\",  # \"power\", \"exponent\", \"equal\"\n",
    "    origin_weights=True,\n",
    "    closest_destination=False,\n",
    "    destination_weights=True, \n",
    "    # perceived_distance=False,\n",
    "    num_cores=8,\n",
    "    light_graph=True,\n",
    "    turn_penalty=False,\n",
    ")\n",
    "simulated_sum_of_flow = return_dict['edge_gdf']['betweenness'].sum()\n",
    "\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t Betweenness estimated\")\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_dict['edge_gdf']\n",
    "\n",
    "\n",
    "\n",
    "joined_results = harvard_square.layers['streets'].gdf.join(\n",
    "    return_dict['edge_gdf'][['parent_street_id', 'betweenness']].set_index('parent_street_id'))#.rename(\n",
    "    #columns={\n",
    "        #\"betweenness\": f\"{network_weight}_{'with_turns' if turn_penalty else 'no_turns'}_{'elastic_weight' if elastic_weight else 'unadjusted_weight'}_{pairing['Between_Name']}\"})\n",
    "\n",
    "\n",
    "\n",
    "joined_results[['__GUID', 'betweenness', 'geometry']].to_csv('2023-07-07 manhattan betweenness flow test.csv')\n",
    "joined_results[['__GUID', 'betweenness', 'geometry']].to_file('2023-07-07 manhattan betweenness flow test.geoJSON', driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_results = harvard_square.layers['streets'].gdf.join(harvard_square.network.edges[['parent_street_id', 'betweenness']].set_index('parent_street_id'))\n",
    "joined_results[['__GUID', 'betweenness', 'geometry']].to_csv('2023-07-07 manhattan betweenness flow test.csv')\n",
    "joined_results[['__GUID', 'betweenness', 'geometry']].to_file('2023-07-07 manhattan betweenness flow test.geoJSON', driver=\"GeoJSON\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "madina_env_latest_updates",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
