{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tuesday 2023-07-11 action items:\n",
    "* ~~Debug Beirut, see where the source of decreased knn weight~~\n",
    "* ~~re run an accessibility simulation, as well as a betweenness flow simulation for beirut~~\n",
    "* ~~Once simulation accessibility results done, send to danie~~ addd flows to folder and send another email\n",
    "* ~~create visuak maos for scenarios showing increase in reach.. Add to google slides~~\n",
    "* ~~Homes to > transit - parks - amenities - schools~~\n",
    "* ~~geometric - no turns, no elastic weight, no decay ....~~\n",
    "* ~~la_extended16_manualFixed~~\n",
    "* ~~Run Somerville~~\n",
    "* ~~RUn LA~~\n",
    "* ~~look at Andres' email about brooklyn , run a simulation using the old code in the workstation~~\n",
    "* ~~look at Lui's email, see if I need to do anything about it~~\n",
    "* ~~create OD pairing table for LA, send to Andres~~\n",
    "* ~~RUN A FKOW MODEL FOR HOMES TO MOSKS, USING THE SAME PARAMETERS, WITH KNN , FOR ALL SCENARIOS, SEND TO DANIEL~~\n",
    "* ~~CLip lui's dataset into the area that daniel chose, share with andres~~\n",
    "*~~ FOR the clip, use this file \"C:\\Users\\abdul\\Dropbox (MIT)\\115_NYCWalks\\03_Data\\02_Edited\\NYC_Sidewalk_Edit_July\\BK_clipping.geojson\"~~\n",
    "* ~~make sure the new node insertion code works and matches well~~\n",
    "\n",
    "## effecient node edge creation\n",
    "* ~~Make sure the functionality to construct nodes-edges works for a safey buffer~~\n",
    "* ~~Isolate the node insertion code into script~~\n",
    "* ~~commit script to my branch~~\n",
    "* ~~email orion about it, mention how easy to integrate.~~\n",
    "* ~~look into issue with redundant edges~~\n",
    "\n",
    "## Testing\n",
    "* ~~structure tests so its a one csv: first n rows are settings, next m rows are edges flows, with first column being same segment ids from network file~~\n",
    "* ~~standard test case is four files: network.jeojson - origins.geojson - destination.jeojson - testflows.csv~~\n",
    "* ~~function reads a csv into list of dicts: test settings and series of output.~~\n",
    "* ~~build Harvard Square testflows.csv~~\n",
    "~~* make sure this config runs on Harvard Square~~\n",
    "\n",
    "\n",
    "\n",
    "# TODO\n",
    "## Functionality List 1\n",
    "* add visualization functionality to new version of Madina so testing is easier\n",
    "* in network, add  connector segments as a separate layer (Maybe not very needed if origins and destinations visually are snapoped...)\n",
    "* Commit visualization updates to github\n",
    "\n",
    "\n",
    "## Testing\n",
    "* digitize three test cases for manhattan, visualize OD snapping, show where differences (if any) come from between Rhino  and Python in the NYC case.\n",
    "* Put maps into slides\n",
    "* CHeck somerville, to see if we delete the same number of edges, with and without the effecient code \"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\Cities\\Somerville\\Data\\Somerville_network_1000m_buffer.geojson\"\n",
    "\n",
    "## Functionality List 2\n",
    "* build the betweenness simulation workflow, combine with accissibility\n",
    "* include Logger as a class, and zonal would have an instance of that class. Logger handles event documentation, and captures output.\n",
    "* for each origin, have an origin record, showing its knn weight, reach, gravity towards each of its destination, header rows showing settings and parameters\n",
    "* for the network, columns of od flows, headed  by settings and parameters rows that explicitly detail units, weight type, calibration status, .. any relevant data, settings and parameters..\n",
    "* set up a slideshow/diagram to show the relationship between different compoonents of the library.\n",
    "* Set up a notebook to go over estimating flows from one origin to one destination, Document all relevant settings\n",
    "* show how this generalizes to the pairings.csv, iterating over moodel settings, iterating over scenarios\n",
    "* Make a finalized github commit including testing script, betweenness flow notebook. \n",
    "* Deploy to workstation.\n",
    "\n",
    "\n",
    "## Functionality List 3\n",
    "* given the simulation results, and the censor locations, find a way to calibrate counts and generate beta coeffecients (Beirut, SOmerville, NYC)\n",
    "* enable Jupyter notebook server access from workstation.\n",
    "* Figure out a way to do the \"Cities\" dropbox folder, people can add data through dropbox, run simulations through Jupyter Server, get results through Dropbox\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Running Cases\n",
    "* Send output of LA to Nico when done\n",
    "* send Brooklyn output when done\n",
    "* be prepared to run entire NYC simulation, be mindful that daniel already sent more network parts\n",
    "\n",
    "\n",
    "## Map Cleaning\n",
    "https://www.dropbox.com/scl/fo/d6ugcyt3aanfb3yk4n9va/h?dl=0&rlkey=1c2giilvzoyygcxrlry7fscr9\n",
    "\n",
    "\n",
    "\n",
    "! pip install tqdm\n",
    "#from tqdm import tqdm\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test name                      | madina_flow_sum | Rhino_flow_sum  | sum_diff | sum_smlr_% | sle_mean | sle_median | sle_max  | sle>0.1% | sle>1.0% | sle>3.0% | sle>5.0%\n",
      "_Betweenness1                  |          704.85 |          702.07 |     2.78 |     99.60% |  0.0376% |    0.0000% |  2.4893% |        2 |        2 |        0 |        0\n",
      "_Betweenness2                  |        19116.27 |        19025.63 |    90.64 |     99.52% |  0.0439% |    0.0000% |  3.1837% |        2 |        2 |        1 |        0\n",
      "_Betweenness3                  |        19299.00 |        19214.00 |    85.00 |     99.56% |  0.0295% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0\n",
      "_Betweenness4                  |        20938.18 |        20853.18 |    85.00 |     99.59% |  0.0206% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0\n",
      "_Betweenness5                  |        13478.14 |        13394.55 |    83.59 |     99.38% |  0.0288% |    0.0000% |  3.8299% |        1 |        1 |        1 |        0\n",
      "_Betweenness6                  |         7054.29 |         7416.62 |  -362.33 |    104.89% |     inf% |   29.5873% |     inf% |       73 |       73 |       70 |       69\n",
      "Somerville_Bus_Subway_Geometri |          709.44 |          713.14 |    -3.69 |    100.52% |  0.5027% |    0.1954% |  3.9604% |      149 |       62 |        3 |        0\n",
      "Somerville_Bus_Subway          |          740.19 |          743.95 |    -3.76 |    100.51% |  0.5074% |    0.2539% |  3.9604% |      173 |       39 |        3 |        0\n",
      "Somerville_Homes_Subway        |       377922.37 |       389383.31 | -11460.94 |    102.94% |     inf% |    3.8409% |     inf% |     2578 |     2232 |     1591 |      997\n",
      "Somerville_Jobs_Subway         |       146309.13 |       149908.30 | -3599.17 |    102.40% |     inf% |    3.5776% |     inf% |     2078 |     1741 |     1286 |      824\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Layer with label Amenities is already in Zonal object'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 168\u001b[0m\n\u001b[0;32m    158\u001b[0m harvard_square\u001b[39m.\u001b[39mload_layer(\n\u001b[0;32m    159\u001b[0m     layer_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstreets\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    160\u001b[0m     file_path\u001b[39m=\u001b[39m  test_case_folder \u001b[39m+\u001b[39m test_config\u001b[39m.\u001b[39mat[test_idx, \u001b[39m'\u001b[39m\u001b[39mNetwork_File\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m harvard_square\u001b[39m.\u001b[39mload_layer(\n\u001b[0;32m    164\u001b[0m     layer_name\u001b[39m=\u001b[39mtest_config\u001b[39m.\u001b[39mat[test_idx, \u001b[39m'\u001b[39m\u001b[39mOrigin_Name\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    165\u001b[0m     file_path\u001b[39m=\u001b[39m test_case_folder \u001b[39m+\u001b[39m test_config\u001b[39m.\u001b[39mat[test_idx, \u001b[39m'\u001b[39m\u001b[39mOrigin_File\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    166\u001b[0m     )\n\u001b[1;32m--> 168\u001b[0m harvard_square\u001b[39m.\u001b[39;49mload_layer(\n\u001b[0;32m    169\u001b[0m     layer_name\u001b[39m=\u001b[39;49mtest_config\u001b[39m.\u001b[39;49mat[test_idx, \u001b[39m'\u001b[39;49m\u001b[39mDestination_Name\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    170\u001b[0m     file_path\u001b[39m=\u001b[39;49m test_case_folder \u001b[39m+\u001b[39;49m test_config\u001b[39m.\u001b[39;49mat[test_idx, \u001b[39m'\u001b[39;49m\u001b[39mDestination_File\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m    171\u001b[0m     )\n\u001b[0;32m    174\u001b[0m harvard_square\u001b[39m.\u001b[39mcreate_street_network(\n\u001b[0;32m    175\u001b[0m     source_layer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstreets\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m    176\u001b[0m     discard_redundant_edges\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    177\u001b[0m     node_snapping_tolerance\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m,  \u001b[39m# TODO: check for sensitivity... pick one as default snapping.\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     weight_attribute\u001b[39m=\u001b[39mtest_config\u001b[39m.\u001b[39mat[test_idx, \u001b[39m'\u001b[39m\u001b[39mNetwork_Cost\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mif\u001b[39;00m test_config\u001b[39m.\u001b[39mat[test_idx, \u001b[39m'\u001b[39m\u001b[39mNetwork_Cost\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGeometric\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    179\u001b[0m )\n\u001b[0;32m    181\u001b[0m \u001b[39m#print (f\"{harvard_square.network.edges.shape = }\")\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[39m#print (f\"{harvard_square.network.nodes.shape = }\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\..\\madina\\zonal\\zonal.py:69\u001b[0m, in \u001b[0;36mZonal.load_layer\u001b[1;34m(self, layer_name, file_path, pos, first, before, after)\u001b[0m\n\u001b[0;32m     60\u001b[0m gdf \u001b[39m=\u001b[39m _prepare_geometry(gdf)\n\u001b[0;32m     62\u001b[0m layer \u001b[39m=\u001b[39m Layer(\n\u001b[0;32m     63\u001b[0m     layer_name,\n\u001b[0;32m     64\u001b[0m     gdf, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m     file_path\n\u001b[0;32m     68\u001b[0m )\n\u001b[1;32m---> 69\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49madd(\n\u001b[0;32m     70\u001b[0m     layer,\n\u001b[0;32m     71\u001b[0m     pos,\n\u001b[0;32m     72\u001b[0m     first,\n\u001b[0;32m     73\u001b[0m     before,\n\u001b[0;32m     74\u001b[0m     after\n\u001b[0;32m     75\u001b[0m )\n\u001b[0;32m     77\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeo_center:\n\u001b[0;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[0;32m     79\u001b[0m         \u001b[39m# This is to ignore a warning issued for dpoing calculations in a geographic coordinate system, but that's the needed output:\u001b[39;00m\n\u001b[0;32m     80\u001b[0m         \u001b[39m# a point in a geographic coordinate system to center the visualization\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\..\\madina\\zonal\\layer.py:116\u001b[0m, in \u001b[0;36mLayers.add\u001b[1;34m(self, layer, pos, first, before, after)\u001b[0m\n\u001b[0;32m    113\u001b[0m     pos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minsert_at(pos, layer)\n\u001b[1;32m--> 116\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minsert_at(\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers), layer)\n\u001b[0;32m    117\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\..\\madina\\zonal\\layer.py:133\u001b[0m, in \u001b[0;36mLayers.insert_at\u001b[1;34m(self, pos, layer)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpos\u001b[39m}\u001b[39;00m\u001b[39m must have an int type\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mlabel \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_layers:\n\u001b[1;32m--> 133\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLayer with label \u001b[39m\u001b[39m{\u001b[39;00mlayer\u001b[39m.\u001b[39mlabel\u001b[39m}\u001b[39;00m\u001b[39m is already in Zonal object\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    135\u001b[0m label \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mlabel\n\u001b[0;32m    137\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39minsert(pos, label)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Layer with label Amenities is already in Zonal object'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "\n",
    "from madina.zonal.zonal import Zonal\n",
    "from madina.una.betweenness import parallel_betweenness\n",
    "from madina.una.elastic import get_elastic_weight\n",
    "\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "from geopandas import GeoDataFrame, GeoSeries\n",
    "\n",
    "\n",
    "print (f\"{'test name':30s} | {'madina_flow_sum':15s} | {'Rhino_flow_sum':15s} | {'sum_diff':8s} | {'sum_smlr_%':10s} | {'sle_mean':8s} | {'sle_median':10s} | {'sle_max':8s} | {'sle>0.1%':8s} | {'sle>1.0%':8s} | {'sle>3.0%':8s} | {'sle>5.0%':8s}\")\n",
    "#for test_case in os.listdir(\"Test Cases\"):\n",
    "for test_case in ['Harvard Square', 'Somerville']:   #\n",
    "    # TODO: Check OS compatibility, ensure this is compatible with Unix systems..\n",
    "    test_case_folder = \"Test Cases\" + \"\\\\\" + test_case + \"\\\\\"\n",
    "    test_config = pd.read_csv(test_case_folder + \"test_configs.csv\")\n",
    "    test_flows =  pd.read_csv(test_case_folder + \"test_flows.csv\")\n",
    "\n",
    "    if test_case == 'Somerville':\n",
    "        resdy_tests = test_config.index[0:4]\n",
    "    else:\n",
    "        ready_tests = test_config.index\n",
    "\n",
    "    for test_idx in ready_tests:\n",
    "\n",
    "        harvard_square = Zonal()\n",
    "\n",
    "        harvard_square.load_layer(\n",
    "            layer_name='streets',\n",
    "            file_path=  test_case_folder + test_config.at[test_idx, 'Network_File']\n",
    "            )\n",
    "\n",
    "        harvard_square.load_layer(\n",
    "            layer_name=test_config.at[test_idx, 'Origin_Name'],\n",
    "            file_path= test_case_folder + test_config.at[test_idx, 'Origin_File']\n",
    "            )\n",
    "\n",
    "        harvard_square.load_layer(\n",
    "            layer_name=test_config.at[test_idx, 'Destination_Name'],\n",
    "            file_path= test_case_folder + test_config.at[test_idx, 'Destination_File']\n",
    "            )\n",
    "        \n",
    "\n",
    "        harvard_square.create_street_network(\n",
    "            source_layer='streets', \n",
    "            discard_redundant_edges=False,\n",
    "            node_snapping_tolerance=0.01,  # TODO: check for sensitivity... pick one as default snapping.\n",
    "            weight_attribute=test_config.at[test_idx, 'Network_Cost'] if test_config.at[test_idx, 'Network_Cost'] != \"Geometric\" else None\n",
    "        )\n",
    "\n",
    "        #print (f\"{harvard_square.network.edges.shape = }\")\n",
    "        #print (f\"{harvard_square.network.nodes.shape = }\")\n",
    "        harvard_square.network.nodes,  harvard_square.network.edges = _discard_redundant_edges(harvard_square.network.nodes, harvard_square.network.edges)\n",
    "        #print (f\"{harvard_square.network.edges.shape = }\")\n",
    "        #print (f\"{harvard_square.network.nodes.shape = }\")\n",
    "\n",
    "        harvard_square.insert_node(\n",
    "            layer_name=test_config.at[test_idx, 'Origin_Name'], \n",
    "            label='origin', \n",
    "            weight_attribute=test_config.at[test_idx, 'Origin_Weight'] if test_config.at[test_idx, 'Origin_Weight'] != \"Count\" else None\n",
    "        )\n",
    "        harvard_square.insert_node(\n",
    "            layer_name=test_config.at[test_idx, 'Destination_Name'], \n",
    "            label='destination', \n",
    "            weight_attribute=test_config.at[test_idx, 'Destination_Weight'] if test_config.at[test_idx, 'Destination_Weight'] != \"Count\" else None\n",
    "        )\n",
    "\n",
    "        harvard_square.create_graph()\n",
    "\n",
    "        node_gdf = harvard_square.network.nodes\n",
    "        origin_gdf = node_gdf[node_gdf['type'] == 'origin']\n",
    "\n",
    "        harvard_square.network.nodes[\"original_weight\"] = harvard_square.network.nodes[\"weight\"]\n",
    "\n",
    "        harvard_square.network.turn_penalty_amount = test_config.at[test_idx, 'Turn penalty']\n",
    "        harvard_square.network.turn_threshold_degree = test_config.at[test_idx, 'Turn threshold']\n",
    "\n",
    "        if test_config.at[test_idx, 'Elastic_weights']:\n",
    "            harvard_square.network.nodes[\"weight\"] = harvard_square.network.nodes[\"original_weight\"]\n",
    "            get_elastic_weight(\n",
    "                harvard_square.network,\n",
    "                search_radius=test_config.at[test_idx, 'Radius'],\n",
    "                detour_ratio=test_config.at[test_idx, 'Detour'],\n",
    "                beta=test_config.at[test_idx, 'Beta'],\n",
    "                decay=True, #test_config.at[test_idx, 'Decay'],\n",
    "                #turn_penalty=test_config.at[test_idx, 'Turns'],\n",
    "                turn_penalty=False,\n",
    "            )\n",
    "            for o_idx in origin_gdf.index:\n",
    "                harvard_square.network.nodes.at[o_idx, 'weight'] =  harvard_square.network.nodes.at[o_idx, 'elastic_weight']\n",
    "\n",
    "\n",
    "        return_dict = parallel_betweenness(\n",
    "            harvard_square.network,\n",
    "            search_radius=test_config.at[test_idx, 'Radius'],\n",
    "            detour_ratio=test_config.at[test_idx, 'Detour'],\n",
    "            decay=test_config.at[test_idx, 'Decay'], #if test['Elastic weights'] else True,\n",
    "            decay_method=test_config.at[test_idx, 'Decay_Mode'],  # \"power\", \"exponent\"\n",
    "            beta=test_config.at[test_idx, 'Beta'],\n",
    "            path_detour_penalty='equal', # \"equal\",  # \"power\", \"exponent\", \"equal\"\n",
    "            origin_weights=False if type(test_config.at[test_idx, 'Origin_Weight']) != str else True,\n",
    "            closest_destination=test_config.at[test_idx, 'Closest_destination'],\n",
    "            destination_weights=False if type(test_config.at[test_idx, 'Destination_Weight']) != str  else True,    #or (test['Elastic weights'])\n",
    "            # perceived_distance=False,\n",
    "            num_cores=6,\n",
    "            light_graph=True,\n",
    "            turn_penalty=test_config.at[test_idx, 'Turns'],\n",
    "        )\n",
    "        simulated_sum_of_flow = return_dict['edge_gdf']['betweenness'].sum()\n",
    "        test_flow = test_flows[test_config.at[test_idx, 'Flow_Name']].sum()\n",
    "\n",
    "\n",
    "        ## create segment level comparison\n",
    "        # creating connector lines\n",
    "\n",
    "        import shapely.geometry as geo\n",
    "        simulated_betweenness = return_dict['edge_gdf'][['betweenness', 'parent_street_id']].rename(columns={\"betweenness\": \"simulated_betweenness\"}).drop_duplicates(subset=['parent_street_id']).set_index(\"parent_street_id\")\n",
    "        simulated_betweenness = harvard_square.layers[\"streets\"].gdf[[\"geometry\", \"__GUID\"]].join(simulated_betweenness).set_index(\"__GUID\")\n",
    "\n",
    "        test_name = test_config.at[test_idx, 'Flow_Name']\n",
    "        test_betweenness = test_flows[['__GUID', test_name]].set_index(\"__GUID\").rename(columns = {test_name: \"test_flow\"})\n",
    "\n",
    "\n",
    "        comparison = simulated_betweenness.join(test_betweenness)\n",
    "        comparison[\"difference\"] = comparison[\"simulated_betweenness\"] - comparison[\"test_flow\"]\n",
    "        comparison[\"difference_pct\"] = abs(comparison[\"difference\"]) / comparison[\"simulated_betweenness\"] *100\n",
    "        # segment level error\n",
    "        sle = comparison[~comparison[\"difference_pct\"].isna()]['difference_pct']\n",
    "\n",
    "        #'''\n",
    "        node_gdf = harvard_square.network.nodes\n",
    "        edge_gdf = harvard_square.network.edges\n",
    "\n",
    "\n",
    "        origin_layer = harvard_square.layers[test_config.at[test_idx, 'Origin_Name']].gdf\n",
    "        origin_nodes = node_gdf[node_gdf['type'] == 'origin']\n",
    "        origin_joined = origin_layer.join(origin_nodes.set_index('source_id'),lsuffix='_origin')\n",
    "        origin_joined['connector_line'] = origin_joined.apply(lambda x:geo.LineString([x['geometry'], x[\"geometry_origin\"]]), axis=1)\n",
    "        origin_joined[\"geometry\"] = origin_joined['connector_line']\n",
    "\n",
    "\n",
    "        destination_layer = harvard_square.layers[test_config.at[test_idx, 'Destination_Name']].gdf\n",
    "        destination_nodes = node_gdf[node_gdf['type'] == 'destination']\n",
    "        destination_joined = destination_layer.join(destination_nodes.set_index('source_id'),lsuffix='_destination')\n",
    "        destination_joined['connector_line'] = destination_joined.apply(lambda x:geo.LineString([x['geometry'], x[\"geometry_destination\"]]), axis=1)\n",
    "        destination_joined[\"geometry\"] = destination_joined['connector_line']\n",
    "\n",
    "        streets = harvard_square.layers[\"streets\"].gdf \n",
    "        network_file = gpd.read_file(test_case_folder + test_config.at[test_idx, 'Network_File'], engine='pyogrio')\n",
    "\n",
    "\n",
    "        flow_difference = comparison[\n",
    "            ((comparison['test_flow' ] > 0) & (comparison['simulated_betweenness'] == 0)) | \n",
    "            ((comparison['test_flow' ] == 0) & (comparison['simulated_betweenness'] > 0))\n",
    "        ]\n",
    "        harvard_square.create_map(\n",
    "            [\n",
    "                #{'gdf': streets[streets[test_config.at[test_idx, 'Network_Cost']] > 0], 'color': [255, 255, 255], 'text': test_config.at[test_idx, 'Network_Cost']},\n",
    "                {'gdf': streets, 'color': [100, 100, 100], 'opacity': 0.1},\n",
    "                {'gdf': edge_gdf[edge_gdf['betweenness'] > 0], 'color': ['125, 125, 0'], 'text': 'betweenness', 'opacity': 0.2},\n",
    "                #{'gdf': comparison[abs(comparison[\"difference\"]) > 0.01], 'color_by_attribute': 'difference', 'color_method': 'gradient', 'text': 'difference'},\n",
    "                {'gdf': comparison[(comparison[\"difference_pct\"] >= 0.1) & (comparison[\"difference_pct\"] < 100)], 'color_by_attribute': 'difference_pct', 'color_method': 'gradient', 'text': 'difference_pct'},\n",
    "                {'gdf': edge_gdf[edge_gdf['snapped'] == True], 'color': [255, 0, 255], 'opacity': 0.2},\n",
    "                {'gdf': network_file[network_file[\"geometry\"].geom_type == 'Polygon'], 'color': [125, 0, 125], 'text': '__GUID'},\n",
    "                {'gdf': flow_difference, 'color': [255, 255, 0] , 'text': 'difference'},        \n",
    "                #{'gdf': comparison[comparison['difference'] != 0], 'color_by_attribute': 'difference', 'color_method': 'gradient', 'text': 'difference', 'opacity':  0.1},\n",
    "                {'gdf': origin_layer, 'color': [0, 0, 255]},\n",
    "                {'gdf': origin_joined[['geometry']], 'color': [0, 0, 255]},\n",
    "                {'gdf': destination_layer, 'color': [255, 0, 0]},\n",
    "                {'gdf': destination_joined[['geometry']], 'color': [255, 0, 0]},\n",
    "                #{'gdf': harvard_square.network.nodes[['geometry', 'type', 'weight']].reset_index(), 'color': [255, 0, 255], 'text': 'id'},\n",
    "            ],\n",
    "            save_as=\"Test Cases\\\\\" + test_case + '\\\\'  + test_name + \"._difference_map.html\"\n",
    "        )\n",
    "        #'''\n",
    "        #print (test_config.loc[test_idx])\n",
    "        print (f\"{test_config.at[test_idx, 'Flow_Name'][:30]:30s} | {simulated_sum_of_flow:15.2f} | {test_flow:15.2f} | {simulated_sum_of_flow - test_flow:8.2f} | {1-(simulated_sum_of_flow - test_flow)/ test_flow:10.2%} | {sle.mean():7.4f}% | {sle.median():9.4f}% | {sle.max():7.4f}% | {sle[sle > 0.1].count():8} | {sle[sle > 1.0].count():8} | {sle[sle > 3.0].count():8} | {sle[sle > 5.0].count():8}\")\n",
    "        #print (\"DOne Case...\")\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "With splitting redundant nodes. low tolerance = 0.000001\n",
    "\n",
    "\n",
    "test name                      | madina_flow_sum | Rhino_flow_sum  | sum_diff | sum_smlr_% | sle_mean | sle_median | sle_max  | sle>0.1% | sle>1.0% | sle>3.0% | sle>5.0%\n",
    "Somerville_Bus_Subway_Geometri |          709.44 |          713.14 |    -3.69 |    100.52% |  0.5027% |    0.1954% |  3.9604% |      149 |       62 |        3 |        0\n",
    "Somerville_Bus_Subway          |          740.19 |          743.95 |    -3.76 |    100.51% |  0.5074% |    0.2539% |  3.9604% |      173 |       39 |        3 |        0\n",
    "Somerville_Homes_Subway        |       372834.66 |       389383.31 | -16548.65 |    104.25% |     inf% |    3.4592% |     inf% |     2562 |     2191 |     1467 |      973\n",
    "Somerville_Jobs_Subway         |       142623.89 |       149908.30 | -7284.41 |    104.86% |     inf% |    3.3898% |     inf% |     2077 |     1691 |     1237 |      854"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "      <th>simulated_betweenness</th>\n",
       "      <th>test_flow</th>\n",
       "      <th>difference</th>\n",
       "      <th>difference_pct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__GUID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9c2a0a1e-93d0-433d-8ef7-28dbd6399747</th>\n",
       "      <td>LINESTRING (230790.763 905654.025, 230779.586 ...</td>\n",
       "      <td>68.441179</td>\n",
       "      <td>68.513274</td>\n",
       "      <td>-0.072095</td>\n",
       "      <td>0.105338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2716b82e-dd5a-4d56-9600-f1be9ca7dc65</th>\n",
       "      <td>LINESTRING (235426.555 902244.112, 235340.448 ...</td>\n",
       "      <td>249.194789</td>\n",
       "      <td>249.472320</td>\n",
       "      <td>-0.277532</td>\n",
       "      <td>0.111371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16900608-afb7-4a1f-ad89-2c409b1f787f</th>\n",
       "      <td>LINESTRING (231614.209 904322.518, 231610.231 ...</td>\n",
       "      <td>2.617736</td>\n",
       "      <td>2.620768</td>\n",
       "      <td>-0.003033</td>\n",
       "      <td>0.115846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5029006c-04ae-468c-9864-57d94d93f9c7</th>\n",
       "      <td>LINESTRING (230943.967 904147.430, 230959.739 ...</td>\n",
       "      <td>19.276246</td>\n",
       "      <td>19.299348</td>\n",
       "      <td>-0.023102</td>\n",
       "      <td>0.119849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44fe89a6-f0dc-4f6a-9248-c7eb987d307b</th>\n",
       "      <td>LINESTRING (230958.695 904156.903, 231094.268 ...</td>\n",
       "      <td>19.205963</td>\n",
       "      <td>19.229048</td>\n",
       "      <td>-0.023086</td>\n",
       "      <td>0.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62f07ef0-7a9e-449a-9f9f-8fb1bf469642</th>\n",
       "      <td>LINESTRING (231089.649 904550.175, 231044.921 ...</td>\n",
       "      <td>5.621143</td>\n",
       "      <td>10.415312</td>\n",
       "      <td>-4.794169</td>\n",
       "      <td>85.288160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb17843e-c50e-4319-a683-9680bd442913</th>\n",
       "      <td>LINESTRING (235119.769 903581.204, 235111.791 ...</td>\n",
       "      <td>0.038618</td>\n",
       "      <td>0.072567</td>\n",
       "      <td>-0.033949</td>\n",
       "      <td>87.909842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1ddb4033-54ce-48b7-821d-ef7a2a43fe5c</th>\n",
       "      <td>LINESTRING (235172.561 903621.278, 235157.705 ...</td>\n",
       "      <td>0.115854</td>\n",
       "      <td>0.217701</td>\n",
       "      <td>-0.101847</td>\n",
       "      <td>87.909843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9871cd4d-0477-476f-9667-8d72422caf59</th>\n",
       "      <td>LINESTRING (235119.769 903581.204, 235116.543 ...</td>\n",
       "      <td>0.077236</td>\n",
       "      <td>0.145134</td>\n",
       "      <td>-0.067898</td>\n",
       "      <td>87.909844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19226c50-b7b1-4cba-aba8-3864716897b5</th>\n",
       "      <td>LINESTRING (230883.583 904997.980, 230836.015 ...</td>\n",
       "      <td>1.781356</td>\n",
       "      <td>3.376132</td>\n",
       "      <td>-1.594776</td>\n",
       "      <td>89.525952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2038 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               geometry  \\\n",
       "__GUID                                                                                    \n",
       "9c2a0a1e-93d0-433d-8ef7-28dbd6399747  LINESTRING (230790.763 905654.025, 230779.586 ...   \n",
       "2716b82e-dd5a-4d56-9600-f1be9ca7dc65  LINESTRING (235426.555 902244.112, 235340.448 ...   \n",
       "16900608-afb7-4a1f-ad89-2c409b1f787f  LINESTRING (231614.209 904322.518, 231610.231 ...   \n",
       "5029006c-04ae-468c-9864-57d94d93f9c7  LINESTRING (230943.967 904147.430, 230959.739 ...   \n",
       "44fe89a6-f0dc-4f6a-9248-c7eb987d307b  LINESTRING (230958.695 904156.903, 231094.268 ...   \n",
       "...                                                                                 ...   \n",
       "62f07ef0-7a9e-449a-9f9f-8fb1bf469642  LINESTRING (231089.649 904550.175, 231044.921 ...   \n",
       "cb17843e-c50e-4319-a683-9680bd442913  LINESTRING (235119.769 903581.204, 235111.791 ...   \n",
       "1ddb4033-54ce-48b7-821d-ef7a2a43fe5c  LINESTRING (235172.561 903621.278, 235157.705 ...   \n",
       "9871cd4d-0477-476f-9667-8d72422caf59  LINESTRING (235119.769 903581.204, 235116.543 ...   \n",
       "19226c50-b7b1-4cba-aba8-3864716897b5  LINESTRING (230883.583 904997.980, 230836.015 ...   \n",
       "\n",
       "                                      simulated_betweenness   test_flow  \\\n",
       "__GUID                                                                    \n",
       "9c2a0a1e-93d0-433d-8ef7-28dbd6399747              68.441179   68.513274   \n",
       "2716b82e-dd5a-4d56-9600-f1be9ca7dc65             249.194789  249.472320   \n",
       "16900608-afb7-4a1f-ad89-2c409b1f787f               2.617736    2.620768   \n",
       "5029006c-04ae-468c-9864-57d94d93f9c7              19.276246   19.299348   \n",
       "44fe89a6-f0dc-4f6a-9248-c7eb987d307b              19.205963   19.229048   \n",
       "...                                                     ...         ...   \n",
       "62f07ef0-7a9e-449a-9f9f-8fb1bf469642               5.621143   10.415312   \n",
       "cb17843e-c50e-4319-a683-9680bd442913               0.038618    0.072567   \n",
       "1ddb4033-54ce-48b7-821d-ef7a2a43fe5c               0.115854    0.217701   \n",
       "9871cd4d-0477-476f-9667-8d72422caf59               0.077236    0.145134   \n",
       "19226c50-b7b1-4cba-aba8-3864716897b5               1.781356    3.376132   \n",
       "\n",
       "                                      difference  difference_pct  \n",
       "__GUID                                                            \n",
       "9c2a0a1e-93d0-433d-8ef7-28dbd6399747   -0.072095        0.105338  \n",
       "2716b82e-dd5a-4d56-9600-f1be9ca7dc65   -0.277532        0.111371  \n",
       "16900608-afb7-4a1f-ad89-2c409b1f787f   -0.003033        0.115846  \n",
       "5029006c-04ae-468c-9864-57d94d93f9c7   -0.023102        0.119849  \n",
       "44fe89a6-f0dc-4f6a-9248-c7eb987d307b   -0.023086        0.120200  \n",
       "...                                          ...             ...  \n",
       "62f07ef0-7a9e-449a-9f9f-8fb1bf469642   -4.794169       85.288160  \n",
       "cb17843e-c50e-4319-a683-9680bd442913   -0.033949       87.909842  \n",
       "1ddb4033-54ce-48b7-821d-ef7a2a43fe5c   -0.101847       87.909843  \n",
       "9871cd4d-0477-476f-9667-8d72422caf59   -0.067898       87.909844  \n",
       "19226c50-b7b1-4cba-aba8-3864716897b5   -1.594776       89.525952  \n",
       "\n",
       "[2038 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".sort_values('difference_pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_difference = comparison[\n",
    "    ((comparison['test_flow' ] > 0) & (comparison['simulated_betweenness'] == 0)) | \n",
    "    ((comparison['test_flow' ] == 0) & (comparison['simulated_betweenness'] > 0))\n",
    "]\n",
    "harvard_square.create_map(\n",
    "    [\n",
    "        #{'gdf': streets[streets[test_config.at[test_idx, 'Network_Cost']] > 0], 'color': [255, 255, 255], 'text': test_config.at[test_idx, 'Network_Cost']},\n",
    "        {'gdf': streets, 'color': [100, 100, 100], 'opacity': 0.1},\n",
    "        {'gdf': edge_gdf[edge_gdf['betweenness'] > 0], 'color': ['125, 125, 125'], 'text': 'betweenness', 'opacity': 0.1},\n",
    "        {'gdf': comparison[abs(comparison[\"difference\"]) > 0.01], 'color_by_attribute': 'difference', 'color_method': 'gradient', 'text': 'difference'},\n",
    "        {'gdf': edge_gdf[edge_gdf['snapped'] == True], 'color': [255, 0, 255], 'text': 'snapping_distance'},\n",
    "        {'gdf': network_file[network_file[\"geometry\"].geom_type == 'Polygon'], 'color': [125, 0, 125], 'text': '__GUID'},\n",
    "        {'gdf': flow_difference, 'color': [255, 255, 0] , 'text': 'difference'},        \n",
    "        #{'gdf': comparison[comparison['difference'] != 0], 'color_by_attribute': 'difference', 'color_method': 'gradient', 'text': 'difference', 'opacity':  0.1},\n",
    "        {'gdf': origin_layer, 'color': [0, 0, 255]},\n",
    "        {'gdf': origin_joined[['geometry']], 'color': [0, 0, 255]},\n",
    "        {'gdf': destination_layer, 'color': [255, 0, 0]},\n",
    "        {'gdf': destination_joined[['geometry']], 'color': [255, 0, 0]},\n",
    "        #{'gdf': harvard_square.network.nodes[['geometry', 'type', 'weight']].reset_index(), 'color': [255, 0, 255], 'text': 'id'},\n",
    "    ],\n",
    "    save_as=\"Test Cases\\\\\" + test_case + '\\\\'  + test_name + \"._difference_map.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard_square.layers.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard_square.network.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard_square.network.edges.set_index(\"parent_street_id\").loc[5759]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_file[network_file[\"geometry\"].geom_type == 'Polygon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_file[(network_file[\"PercLen\"] < 10) & (network_file[\"PercLen\"] > 0)][\"PercLen\"]\n",
    "network_file[network_file[\"__Length\"] < 0.1]\n",
    "\n",
    "network_file[network_file[\"PercLen\"] > network_file[\"geometry\"].length]\n",
    "network_file[network_file[\"PercLen\"] > 0][['__Length', 'PercLen']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_file[network_file['PercLen'] != 0]\n",
    "network_file['weight'] = network_file.apply(lambda x: x[\"PercLen\"] if x[\"PercLen\"] != 0 else x[\"geometry\"].length, axis=1)\n",
    "network_file[network_file['PercLen'] == 0][['PercLen', 'weight']]\n",
    "weight_series = network_file.apply(lambda x: x[\"PercLen\"] if x[\"PercLen\"] != 0 else x[\"geometry\"].length, axis=1)\n",
    "weight_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "harvard_square.layers['streets'].gdf['PercLen'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_gdf = harvard_square.network.edges\n",
    "edge_gdf[edge_gdf['weight'] <= 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_nodes\n",
    "destination_nodes\n",
    "test_config.at[test_idx, 'Elastic_weights']\n",
    "origin_nodes\n",
    "return_dict['edge_gdf'][\"betweenness\"].max()\n",
    "type(test_config.at[test_idx, 'Elastic_weights'])\n",
    "\n",
    "test_config.at[test_idx, 'Elastic_weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_betweenness['simulated_betweenness'].sum()\n",
    "simulated_betweenness.index.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison[comparison[\"difference\"] < 0]\n",
    "comparison[\"difference\"]\n",
    "test_betweenness.sum()\n",
    "test_betweenness.index.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison[comparison[\"difference_pct\"] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_dict['edge_gdf'][['betweenness', 'parent_street_id']].rename(columns={\"betweenness\": \"simulated_betweenness\"}).set_index(\"parent_street_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how{‘left’, ‘right’, ‘outer’, ‘inner’, ‘cross’}\n",
    "test_betweenness.join(simulated_betweenness, how='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_betweenness = return_dict['edge_gdf'][['betweenness', 'parent_street_id']].rename(columns={\"betweenness\": \"simulated_betweenness\"}).drop_duplicates(subset=['parent_street_id']).set_index(\"parent_street_id\")\n",
    "simulated_betweenness = harvard_square.layers[\"streets\"].gdf[[\"geometry\", \"__GUID\"]].join(simulated_betweenness).set_index(\"__GUID\")\n",
    "\n",
    "test_name = test_config.at[test_idx, 'test_name']\n",
    "test_betweenness = test_flows[['__GUID', test_name]].set_index(\"__GUID\").rename(columns = {test_name: \"test_flow\"})\n",
    "\n",
    "\n",
    "#comparison = simulated_betweenness.join(test_betweenness)\n",
    "comparison = test_betweenness.join(simulated_betweenness)\n",
    "comparison[\"difference\"] = comparison[\"simulated_betweenness\"] - comparison[\"test_flow\"]\n",
    "comparison[\"difference_pct\"] = abs(comparison[\"difference\"]) / comparison[\"simulated_betweenness\"] *100\n",
    "mean_error = comparison[~comparison[\"difference_pct\"].isna()]['difference_pct'].mean() \n",
    "\n",
    "list(comparison[~comparison[\"difference_pct\"].isna()]['difference_pct'])\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_betweenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_dict['edge_gdf'][['betweenness', 'parent_street_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COnstructing the Manhattan Case..\n",
    "## Don't forget to check for node insertion, set  return_all=False in the spatial index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "buildings_file = r\"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\Test Cases\\Manhattan\\Home_PT_6538.geojson\"\n",
    "subway_file = r\"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\Test Cases\\Manhattan\\Metro_PT_6538.geojson\"\n",
    "network_file = r\"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\Test Cases\\Manhattan\\network_clipped_dupremovedAS.geojson\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from madina.zonal.zonal import Zonal\n",
    "\n",
    "\n",
    "harvard_square = Zonal()\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t imports done, object created\")\n",
    "start = time.time()\n",
    "\n",
    "#harvard_square.load_layer(\n",
    "#    layer_name='streets',\n",
    "#    file_path=network_file\n",
    "#    )\n",
    "\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t street data loaded\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.load_layer(\n",
    "    layer_name='buildings',\n",
    "    file_path=buildings_file\n",
    "    )\n",
    "\n",
    "buildings_gdf = harvard_square.layers['buildings'].gdf\n",
    "harvard_square.layers['buildings'].gdf = buildings_gdf[~buildings_gdf['FID'].isin([27967, 9140, 3974])]\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t building data loaded\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.load_layer(\n",
    "    layer_name='subway',\n",
    "    file_path=subway_file\n",
    "    )\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t subway data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "harvard_square.create_street_network(\n",
    "    source_layer='streets', \n",
    "    discard_redundant_edges=False, \n",
    "    node_snapping_tolerance=0\n",
    ")\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t street network created\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.insert_node(\n",
    "    layer_name='buildings', \n",
    "    label='origin', \n",
    "    weight_attribute='TotalPop'\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t origins insertes\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.insert_node(\n",
    "    layer_name='subway', \n",
    "    label='destination', \n",
    "    weight_attribute='line_ent_st'\n",
    ")\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t destinations insertes\")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.create_graph(light_graph=True, d_graph=True)\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t graph created insertes\")\n",
    "start = time.time()\n",
    "\n",
    "return_dict = parallel_betweenness(\n",
    "    harvard_square.network,\n",
    "    search_radius=800,\n",
    "    detour_ratio=1.15,\n",
    "    decay=False,\n",
    "    decay_method='exponent',  # \"power\", \"exponent\"\n",
    "    beta=0.004,\n",
    "    path_detour_penalty=\"equal\",  # \"power\", \"exponent\", \"equal\"\n",
    "    origin_weights=True,\n",
    "    closest_destination=False,\n",
    "    destination_weights=True, \n",
    "    # perceived_distance=False,\n",
    "    num_cores=8,\n",
    "    light_graph=True,\n",
    "    turn_penalty=False,\n",
    ")\n",
    "simulated_sum_of_flow = return_dict['edge_gdf']['betweenness'].sum()\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t Betweenness estimated\")\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_results = harvard_square.layers['streets'].gdf.join(harvard_square.network.edges[['parent_street_id', 'betweenness']].set_index('parent_street_id'))\n",
    "joined_results[['__GUID', 'betweenness', 'geometry']].to_csv('2023-07-07 manhattan betweenness flow test.csv')\n",
    "joined_results[['__GUID', 'betweenness', 'geometry']].to_file('2023-07-07 manhattan betweenness flow test.geoJSON', driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'No layer in the Zonal object has the label homes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m harvard_square\u001b[39m.\u001b[39mcreate_map(\n\u001b[0;32m      2\u001b[0m     [\n\u001b[0;32m      3\u001b[0m         \u001b[39m#{\u001b[39;00m\n\u001b[0;32m      4\u001b[0m             \u001b[39m#'layer': 'streets',\u001b[39;00m\n\u001b[0;32m      5\u001b[0m             \u001b[39m#'color': [125, 125, 125],\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         \u001b[39m#},\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         {\n\u001b[1;32m----> 8\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mgdf\u001b[39m\u001b[39m'\u001b[39m: harvard_square\u001b[39m.\u001b[39;49mlayers[\u001b[39m'\u001b[39;49m\u001b[39mhomes\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mgdf,\n\u001b[0;32m      9\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mcolor_by_attribute\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mline_ent_st\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcolor_method\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mgradient\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     11\u001b[0m         }\n\u001b[0;32m     12\u001b[0m     ],\n\u001b[0;32m     13\u001b[0m     save_as\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmap.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\..\\madina\\zonal\\layer.py:22\u001b[0m, in \u001b[0;36mLayers.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[39mif\u001b[39;00m item \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_layers:\n\u001b[0;32m     20\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_layers[item]\n\u001b[1;32m---> 22\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo layer in the Zonal object has the label \u001b[39m\u001b[39m{\u001b[39;00mitem\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(item) \u001b[39m==\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m     25\u001b[0m     label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[item]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'No layer in the Zonal object has the label homes'"
     ]
    }
   ],
   "source": [
    "harvard_square.create_map(\n",
    "    [\n",
    "        #{\n",
    "            #'layer': 'streets',\n",
    "            #'color': [125, 125, 125],\n",
    "        #},\n",
    "        {\n",
    "            'gdf': harvard_square.layers['homes'].gdf,\n",
    "            'color_by_attribute': 'line_ent_st',\n",
    "            \"color_method\": 'gradient'\n",
    "        }\n",
    "    ],\n",
    "    save_as=\"map.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to instal pydeck jupyter nb extension\n",
    "\n",
    "`jupyter nbextension install --sys-prefix --symlink --overwrite --py pydeck`\n",
    "\n",
    "`jupyter nbextension enable --sys-prefix --py pydeck`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "madina_env_latest_updates",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
