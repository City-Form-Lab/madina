{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential Speedup imporvements\n",
    "\n",
    "path generation\n",
    "* remove any use of ROUND()\n",
    "* while returning paths, make sure all nodes in the paths are network nodes (no origin or destination nodes..).\n",
    "\n",
    "subgraph generation\n",
    "* consoliate + weight\n",
    "\n",
    "destination discovery\n",
    "* stop searching for origin nodes past 0.5 search radius\n",
    "* remove duplicate if statements\n",
    "\n",
    "\n",
    "update light graph\n",
    "* shorter list of segments to iterate over\n",
    "* one add one remove\n",
    "\n",
    "\n",
    "betweenness\n",
    "* remove decay calculations from inner loop.\n",
    "* vectorize betweenness calculation outside path loop\n",
    "* reduce calls to node_gdf.at[] by doing them when thwey're first possible, instead of inside path loop\n",
    "* vectorize destination probability calculations\n",
    "\n",
    "\n",
    "\n",
    "path generation\n",
    "* return paths as edges\n",
    "* remove len(neighbors) = 1 check, causes excessive calls to neighbors.\n",
    "* consolidate edge information retreval from graph.\n",
    "\n",
    "betweenness\n",
    "* eliminate edge list construction inside loop.\n",
    "\n",
    "parallel Betweenness:\n",
    "* Implement a queue for parallel processing\n",
    "* better reporting from quue progress\n",
    "\n",
    "Betweenness\n",
    "* update to accomidate queue feeding\n",
    "* implement controls for starting a job only when there is enough memory...\n",
    "* selete large variable prior to next job, so save for memory when waiting for memory to free up\n",
    "\n",
    "path generation:\n",
    "* remove edge sets, sacrificing time performance for memory savings\n",
    "* slightly more optimized scope neighbor, use set operations to filter neighbors in scope, instead of list comprehension with checking..\n",
    "* experament with dequeue for cheaper path appends...\n",
    "* significiant memory redunction by eliminating need to store \"targets remaining\", this might lead to reduction in cpu effeciency?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For next week:\n",
    "* Send rounaq wide table of counts\n",
    "* Once recieved, from lui, start a new digitization process in the new network\n",
    "* look at th eparks file situation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functionality List 3\n",
    "* set up a slideshow/diagram to show the relationship between different compoonents of the library.\n",
    "* Set up a notebook to go over estimating flows from one origin to one destination, Document all relevant settings\n",
    "* show how this generalizes to the pairings.csv, iterating over moodel settings, iterating over scenarios\n",
    "* Make a finalized github commit including testing script, betweenness flow notebook. \n",
    "* Deploy to workstation.\n",
    "* given the simulation results, and the censor locations, find a way to calibrate counts and generate beta coeffecients (Beirut, SOmerville, NYC)\n",
    "* enable Jupyter notebook server access from workstation.\n",
    "* Figure out a way to do the \"Cities\" dropbox folder, people can add data through dropbox, run simulations through Jupyter Server, get results through Dropbox\n",
    "\n",
    "## Map Cleaning\n",
    "https://www.dropbox.com/scl/fo/d6ugcyt3aanfb3yk4n9va/h?dl=0&rlkey=1c2giilvzoyygcxrlry7fscr9\n",
    "\n",
    "\n",
    "\n",
    "## Needed compatibility upgrades:\n",
    "* The query_bulk() method of the spatial index .sindex property is deprecated in favor of query() (#2823).\n",
    "* Make surr there is no direct use of pyGEOS (currently used in node-edge creation if tolerance=0) [ If you use PyGEOS directly and access an array of PyGEOS geometries using GeoSeries.values.data, you will need to make some changes to avoid code breakage.]  , a simple replacement of pygeos with shapely together with a change of gdf.geometry.values.data to gdf.geometry.values or analogous gdf.geometry.array should work\n",
    "\n",
    "\n",
    "## instaling an environment\n",
    "```\n",
    "Run Powershell as administrator\n",
    "\n",
    "conda create --name=madina_0_0_2 python\n",
    "conda activate madina_0_0_2\n",
    "conda config --env --add channels conda-forge\n",
    "conda config --env --set channel_priority strict\n",
    "conda install -f mkl\n",
    "conda install python=3 geopandas\n",
    "conda install pydeck\n",
    "conda install pyogrio\n",
    "conda install rtrees\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test name                      | madina_flow_sum | Rhino_flow_sum  | sum_diff | sum_smlr_% | sle_mean | sle_median | sle_max  | sle>0.1% | sle>1.0% | sle>3.0% | sle>5.0% | time_spent\n",
      "_Betweenness1                  |          702.07 |          702.07 |    -0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |    3.55432\n",
      "_Betweenness2                  |        19025.63 |        19025.63 |     0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |    6.61948\n",
      "_Betweenness3                  |        19214.00 |        19214.00 |     0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |   10.24690\n",
      "_Betweenness4                  |        20853.18 |        20853.18 |    -0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |   13.82649\n",
      "Somerville_Bus_Subway_Geometri |          700.29 |          713.14 |   -12.84 |    101.80% |  2.3488% |    2.2893% |  5.0852% |      266 |      232 |       96 |        2 |   12.57180\n",
      "Somerville_Bus_Subway          |          730.95 |          743.95 |   -13.00 |    101.75% |  2.4117% |    2.2917% |  5.1912% |      285 |      251 |      103 |        8 |   25.89322\n",
      "Somerville_Homes_Subway        |       368786.93 |       389383.31 | -20596.38 |    105.29% |     inf% |    6.9536% |     inf% |     2640 |     2565 |     2355 |     1966 |  283.22346\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from madina.zonal.zonal import Zonal\n",
    "from madina.una.betweenness import parallel_betweenness, paralell_betweenness_exposure, betweenness_exposure\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "\n",
    "\n",
    "print (f\"{'test name':30s} | {'madina_flow_sum':15s} | {'Rhino_flow_sum':15s} | {'sum_diff':8s} | {'sum_smlr_%':10s} | {'sle_mean':8s} | {'sle_median':10s} | {'sle_max':8s} | {'sle>0.1%':8s} | {'sle>1.0%':8s} | {'sle>3.0%':8s} | {'sle>5.0%':8s} | {'time_spent':10s}\")\n",
    "#for test_case in os.listdir(\"Test Cases\"):\n",
    "for test_case in ['Harvard Square', 'Somerville']:   # 'Harvard Square',\n",
    "    start = time.time()\n",
    "    # TODO: Check OS compatibility, ensure this is compatible with Unix systems..\n",
    "    test_case_folder = \"Test Cases\" + \"\\\\\" + test_case + \"\\\\\"\n",
    "    test_config = pd.read_csv(test_case_folder + \"test_configs.csv\")\n",
    "    test_flows =  pd.read_csv(test_case_folder + \"test_flows.csv\")\n",
    "\n",
    "    if test_case == 'Somerville':\n",
    "        ready_tests = test_config.index[0:3]\n",
    "    else:\n",
    "        ready_tests = test_config.index\n",
    "\n",
    "    for test_idx in ready_tests:\n",
    "\n",
    "        harvard_square = Zonal()\n",
    "\n",
    "        harvard_square.load_layer(\n",
    "            layer_name='streets',\n",
    "            file_path=  test_case_folder + test_config.at[test_idx, 'Network_File']\n",
    "            )\n",
    "\n",
    "        harvard_square.load_layer(\n",
    "            layer_name=test_config.at[test_idx, 'Origin_Name'],\n",
    "            file_path= test_case_folder + test_config.at[test_idx, 'Origin_File']\n",
    "            )\n",
    "\n",
    "        harvard_square.load_layer(\n",
    "            layer_name=test_config.at[test_idx, 'Destination_Name'],\n",
    "            file_path= test_case_folder + test_config.at[test_idx, 'Destination_File']\n",
    "            )\n",
    "        \n",
    "\n",
    "        harvard_square.create_street_network(\n",
    "            source_layer='streets', \n",
    "            discard_redundant_edges=True,\n",
    "            split_redundant_edges=False,\n",
    "            node_snapping_tolerance=0.1,  # TODO: check for sensitivity... pick one as default snapping.\n",
    "            weight_attribute=test_config.at[test_idx, 'Network_Cost'] if test_config.at[test_idx, 'Network_Cost'] != \"Geometric\" else None\n",
    "        )\n",
    "\n",
    "        #print (f\"{harvard_square.network.edges.shape = }\")\n",
    "        #print (f\"{harvard_square.network.nodes.shape = }\")\n",
    "        #harvard_square.network.nodes,  harvard_square.network.edges = _discard_redundant_edges(harvard_square.network.nodes, harvard_square.network.edges)\n",
    "        #print (f\"{harvard_square.network.edges.shape = }\")\n",
    "        #print (f\"{harvard_square.network.nodes.shape = }\")\n",
    "\n",
    "        harvard_square.insert_node(\n",
    "            layer_name=test_config.at[test_idx, 'Origin_Name'], \n",
    "            label='origin', \n",
    "            weight_attribute=test_config.at[test_idx, 'Origin_Weight'] if test_config.at[test_idx, 'Origin_Weight'] != \"Count\" else None\n",
    "        )\n",
    "        harvard_square.insert_node(\n",
    "            layer_name=test_config.at[test_idx, 'Destination_Name'], \n",
    "            label='destination', \n",
    "            weight_attribute=test_config.at[test_idx, 'Destination_Weight'] if test_config.at[test_idx, 'Destination_Weight'] != \"Count\" else None\n",
    "        )\n",
    "\n",
    "        harvard_square.create_graph()\n",
    "\n",
    "        node_gdf = harvard_square.network.nodes\n",
    "        origin_gdf = node_gdf[node_gdf['type'] == 'origin']\n",
    "\n",
    "        harvard_square.network.nodes[\"original_weight\"] = harvard_square.network.nodes[\"weight\"]\n",
    "\n",
    "        harvard_square.network.turn_penalty_amount = test_config.at[test_idx, 'Turn_Penalty']\n",
    "        harvard_square.network.turn_threshold_degree = test_config.at[test_idx, 'Turn_Threshold']\n",
    "\n",
    "        if test_config.at[test_idx, 'Elastic_Weights']:\n",
    "            '''\n",
    "            harvard_square.network.nodes[\"weight\"] = harvard_square.network.nodes[\"original_weight\"]\n",
    "            get_elastic_weight(\n",
    "                harvard_square.network,\n",
    "                search_radius=test_config.at[test_idx, 'Radius'],\n",
    "                detour_ratio=test_config.at[test_idx, 'Detour'],\n",
    "                beta=test_config.at[test_idx, 'Beta'],\n",
    "                decay=True, #test_config.at[test_idx, 'Decay'],\n",
    "                #turn_penalty=test_config.at[test_idx, 'Turns'],\n",
    "                turn_penalty=False,\n",
    "            )\n",
    "            for o_idx in origin_gdf.index:\n",
    "                harvard_square.network.nodes.at[o_idx, 'weight'] =  harvard_square.network.nodes.at[o_idx, 'elastic_weight']\n",
    "            '''\n",
    "            continue\n",
    "        '''\n",
    "        return_dict = parallel_betweenness(\n",
    "            harvard_square.network,\n",
    "            search_radius=test_config.at[test_idx, 'Radius'],\n",
    "            detour_ratio=test_config.at[test_idx, 'Detour'],\n",
    "            decay=test_config.at[test_idx, 'Decay'], #if test['Elastic weights'] else True,\n",
    "            decay_method=test_config.at[test_idx, 'Decay_Mode'],  # \"power\", \"exponent\"\n",
    "            beta=test_config.at[test_idx, 'Beta'],\n",
    "            path_detour_penalty='equal', # \"equal\",  # \"power\", \"exponent\", \"equal\"\n",
    "            origin_weights=False if type(test_config.at[test_idx, 'Origin_Weight']) != str else True,\n",
    "            closest_destination=test_config.at[test_idx, 'Closest_Destination'],\n",
    "            destination_weights=False if type(test_config.at[test_idx, 'Destination_Weight']) != str  else True,    #or (test['Elastic weights'])\n",
    "            # perceived_distance=False,\n",
    "            num_cores=6,\n",
    "            light_graph=True,\n",
    "            turn_penalty=test_config.at[test_idx, 'Turns'],\n",
    "        )\n",
    "        \n",
    "        '''\n",
    "        return_dict = paralell_betweenness_exposure(\n",
    "            harvard_square,\n",
    "            search_radius=test_config.at[test_idx,'Radius'],\n",
    "            detour_ratio=test_config.at[test_idx,'Detour'],\n",
    "            decay=False if test_config.at[test_idx,'Elastic_Weights'] else test_config.at[test_idx,'Decay'],  # elastic weight already reduces origin weight factoring in decay. if this pairing uses elastic weights, don't decay again,\n",
    "            decay_method=test_config.at[test_idx,'Decay_Mode'],\n",
    "            beta=test_config.at[test_idx,'Beta'],\n",
    "            num_cores=3,\n",
    "            path_detour_penalty='equal', # \"power\" | \"exponent\" | \"equal\"\n",
    "            closest_destination=test_config.at[test_idx,'Closest_Destination'],\n",
    "            elastic_weight=test_config.at[test_idx,'Elastic_Weights'],\n",
    "            turn_penalty=test_config.at[test_idx,'Turns'],\n",
    "            path_exposure_attribute=None,\n",
    "            return_path_record=False, \n",
    "            destniation_cap=None\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        simulated_sum_of_flow = return_dict['edge_gdf']['betweenness'].sum()\n",
    "        test_flow = test_flows[test_config.at[test_idx, 'Flow_Name']].sum()\n",
    "\n",
    "\n",
    "        ## create segment level comparison\n",
    "        # creating connector lines\n",
    "\n",
    "        import shapely.geometry as geo\n",
    "        simulated_betweenness = return_dict['edge_gdf'][['betweenness', 'parent_street_id']].rename(columns={\"betweenness\": \"simulated_betweenness\"}).drop_duplicates(subset=['parent_street_id']).set_index(\"parent_street_id\")\n",
    "        simulated_betweenness = harvard_square.layers[\"streets\"].gdf[[\"geometry\", \"__GUID\"]].join(simulated_betweenness).set_index(\"__GUID\")\n",
    "\n",
    "        test_name = test_config.at[test_idx, 'Flow_Name']\n",
    "        test_betweenness = test_flows[['__GUID', test_name]].set_index(\"__GUID\").rename(columns = {test_name: \"test_flow\"})\n",
    "\n",
    "\n",
    "        comparison = simulated_betweenness.join(test_betweenness)\n",
    "        comparison[\"difference\"] = comparison[\"simulated_betweenness\"] - comparison[\"test_flow\"]\n",
    "        comparison[\"difference_pct\"] = abs(comparison[\"difference\"]) / comparison[\"simulated_betweenness\"] *100\n",
    "        # segment level error\n",
    "        sle = comparison[~comparison[\"difference_pct\"].isna()]['difference_pct']\n",
    "\n",
    "        \n",
    "        node_gdf = harvard_square.network.nodes\n",
    "        edge_gdf = harvard_square.network.edges\n",
    "\n",
    "\n",
    "        origin_nodes = node_gdf[node_gdf['type'] == 'origin']\n",
    "        '''\n",
    "        origin_layer = harvard_square.layers[test_config.at[test_idx, 'Origin_Name']].gdf\n",
    "        origin_joined = origin_layer.join(origin_nodes.set_index('source_id'),lsuffix='_origin')\n",
    "        origin_joined['connector_line'] = origin_joined.apply(lambda x:geo.LineString([x['geometry'], x[\"geometry_origin\"]]), axis=1)\n",
    "        origin_joined[\"geometry\"] = origin_joined['connector_line']\n",
    "\n",
    "\n",
    "        destination_layer = harvard_square.layers[test_config.at[test_idx, 'Destination_Name']].gdf\n",
    "        destination_nodes = node_gdf[node_gdf['type'] == 'destination']\n",
    "        destination_joined = destination_layer.join(destination_nodes.set_index('source_id'),lsuffix='_destination')\n",
    "        destination_joined['connector_line'] = destination_joined.apply(lambda x:geo.LineString([x['geometry'], x[\"geometry_destination\"]]), axis=1)\n",
    "        destination_joined[\"geometry\"] = destination_joined['connector_line']\n",
    "\n",
    "\n",
    "        #print (f\"{origin_nodes.shape = }\\t{destination_nodes.shape = }\")\n",
    "\n",
    "        streets = harvard_square.layers[\"streets\"].gdf \n",
    "        network_file = gpd.read_file(test_case_folder + test_config.at[test_idx, 'Network_File'], engine='pyogrio')\n",
    "\n",
    "\n",
    "        flow_difference = comparison[\n",
    "            ((comparison['test_flow' ] > 0) & (comparison['simulated_betweenness'] == 0)) | \n",
    "            ((comparison['test_flow' ] == 0) & (comparison['simulated_betweenness'] > 0))\n",
    "        ]\n",
    "        harvard_square.create_map(\n",
    "            [\n",
    "                #{'gdf': streets[streets[test_config.at[test_idx, 'Network_Cost']] > 0], 'color': [255, 255, 255], 'text': test_config.at[test_idx, 'Network_Cost']},\n",
    "                {'gdf': streets, 'color': [100, 100, 100], 'opacity': 0.1},\n",
    "                {'gdf': edge_gdf[edge_gdf['betweenness'] > 0], 'color': ['125, 125, 0'], 'text': 'betweenness', 'opacity': 0.2},\n",
    "                #{'gdf': comparison[abs(comparison[\"difference\"]) > 0.01], 'color_by_attribute': 'difference', 'color_method': 'gradient', 'text': 'difference'},\n",
    "                {'gdf': comparison[(comparison[\"difference_pct\"] >= 0.1) & (comparison[\"difference_pct\"] < 100)], 'color_by_attribute': 'difference_pct', 'color_method': 'gradient', 'text': 'difference_pct'},\n",
    "                {'gdf': edge_gdf[edge_gdf['snapped'] == True], 'color': [255, 0, 255], 'opacity': 0.2},\n",
    "                {'gdf': network_file[network_file[\"geometry\"].geom_type == 'Polygon'], 'color': [125, 0, 125], 'text': '__GUID'},\n",
    "                {'gdf': flow_difference, 'color': [255, 255, 0] , 'text': 'difference'},        \n",
    "                #{'gdf': comparison[comparison['difference'] != 0], 'color_by_attribute': 'difference', 'color_method': 'gradient', 'text': 'difference', 'opacity':  0.1},\n",
    "                {'gdf': origin_layer, 'color': [0, 0, 255]},\n",
    "                {'gdf': origin_joined[['geometry']], 'color': [0, 0, 255]},\n",
    "                {'gdf': destination_layer, 'color': [255, 0, 0]},\n",
    "                {'gdf': destination_joined[['geometry']], 'color': [255, 0, 0]},\n",
    "                {'gdf': harvard_square.network.nodes[['geometry', 'type', 'weight']].reset_index(), 'color': [255, 0, 255], 'text': 'id'},\n",
    "            ],\n",
    "            save_as=\"Test Cases\\\\\" + test_case + '\\\\'  + test_name + \"._difference_map.html\"\n",
    "        )\n",
    "        '''\n",
    "        \n",
    "        #print (test_config.loc[test_idx])\n",
    "        print (f\"{test_config.at[test_idx, 'Flow_Name'][:30]:30s} | {simulated_sum_of_flow:15.2f} | {test_flow:15.2f} | {simulated_sum_of_flow - test_flow:8.2f} | {1-(simulated_sum_of_flow - test_flow)/ test_flow:10.2%} | {sle.mean():7.4f}% | {sle.median():9.4f}% | {sle.max():7.4f}% | {sle[sle > 0.1].count():8} | {sle[sle > 1.0].count():8} | {sle[sle > 3.0].count():8} | {sle[sle > 5.0].count():8} | {time.time() - start: 10.5f}\")\n",
    "        #print (\"DOne Case...\")\n",
    "        #break\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping track of visited destinations\n",
    "test name                      | madina_flow_sum | Rhino_flow_sum  | sum_diff | sum_smlr_% | sle_mean | sle_median | sle_max  | sle>0.1% | sle>1.0% | sle>3.0% | sle>5.0% | time_spent\n",
    "_Betweenness1                  |          702.07 |          702.07 |    -0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |    4.27237\n",
    "_Betweenness2                  |        19025.63 |        19025.63 |     0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |    7.92910\n",
    "_Betweenness3                  |        19214.00 |        19214.00 |     0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |   11.60755\n",
    "_Betweenness4                  |        20853.18 |        20853.18 |    -0.00 |    100.00% |  0.0000% |    0.0000% |  0.0000% |        0 |        0 |        0 |        0 |   15.20730\n",
    "Somerville_Bus_Subway_Geometri |          700.29 |          713.14 |   -12.84 |    101.80% |  2.3488% |    2.2893% |  5.0852% |      266 |      232 |       96 |        2 |   12.55345\n",
    "Somerville_Bus_Subway          |          730.95 |          743.95 |   -13.00 |    101.75% |  2.4117% |    2.2917% |  5.1912% |      285 |      251 |      103 |        8 |   25.87746\n",
    "Somerville_Homes_Subway        |       368786.93 |       389383.31 | -20596.38 |    105.29% |     inf% |    6.9536% |     inf% |     2640 |     2565 |     2355 |     1966 |  277.37418\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOne 100\n",
      "DOne 200\n",
      "DOne 300\n",
      "DOne 400\n",
      "DOne 500\n",
      "DOne 600\n",
      "DOne 700\n",
      "DOne 800\n",
      "DOne 900\n",
      "DOne 1000\n",
      "DOne 1100\n",
      "DOne 1200\n",
      "DOne 1300\n",
      "DOne 1400\n",
      "DOne 1500\n",
      "DOne 1600\n",
      "DOne 1700\n",
      "DOne 1800\n",
      "DOne 1900\n",
      "DOne 2000\n",
      "DOne 2100\n",
      "DOne 2200\n",
      "DOne 2300\n",
      "DOne 2400\n",
      "DOne 2500\n",
      "DOne 2600\n",
      "DOne 2700\n",
      "DOne 2800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 33\u001b[0m\n\u001b[0;32m     14\u001b[0m         return_dict \u001b[39m=\u001b[39m betweenness_exposure(\n\u001b[0;32m     15\u001b[0m             harvard_square,\n\u001b[0;32m     16\u001b[0m             core_index\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m             destniation_cap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     )\n\u001b[0;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m return_dict\n\u001b[1;32m---> 33\u001b[0m x \u001b[39m=\u001b[39m profile_new()\n",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m, in \u001b[0;36mprofile_new\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[39mfor\u001b[39;00m core_index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_cores):\n\u001b[0;32m     11\u001b[0m         origin_queue\u001b[39m.\u001b[39mput(\u001b[39m\"\u001b[39m\u001b[39mdone\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m     return_dict \u001b[39m=\u001b[39m betweenness_exposure(\n\u001b[0;32m     15\u001b[0m         harvard_square,\n\u001b[0;32m     16\u001b[0m         core_index\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     17\u001b[0m         origin_queue\u001b[39m=\u001b[39;49morigin_queue,\n\u001b[0;32m     18\u001b[0m         \u001b[39m#origins=origin_gdf.iloc[:1000],\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m         search_radius\u001b[39m=\u001b[39;49mtest_config\u001b[39m.\u001b[39;49mat[test_idx,\u001b[39m'\u001b[39;49m\u001b[39mRadius\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     20\u001b[0m         detour_ratio\u001b[39m=\u001b[39;49mtest_config\u001b[39m.\u001b[39;49mat[test_idx,\u001b[39m'\u001b[39;49m\u001b[39mDetour\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     21\u001b[0m         decay\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m test_config\u001b[39m.\u001b[39;49mat[test_idx,\u001b[39m'\u001b[39;49m\u001b[39mElastic_Weights\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39melse\u001b[39;49;00m test_config\u001b[39m.\u001b[39;49mat[test_idx,\u001b[39m'\u001b[39;49m\u001b[39mDecay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     22\u001b[0m         beta\u001b[39m=\u001b[39;49mtest_config\u001b[39m.\u001b[39;49mat[test_idx,\u001b[39m'\u001b[39;49m\u001b[39mBeta\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     23\u001b[0m         decay_method\u001b[39m=\u001b[39;49mtest_config\u001b[39m.\u001b[39;49mat[test_idx,\u001b[39m'\u001b[39;49m\u001b[39mDecay_Mode\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     24\u001b[0m         path_detour_penalty\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mequal\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     25\u001b[0m         closest_destination\u001b[39m=\u001b[39;49mtest_config\u001b[39m.\u001b[39;49mat[test_idx,\u001b[39m'\u001b[39;49m\u001b[39mClosest_Destination\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     26\u001b[0m         elastic_weight\u001b[39m=\u001b[39;49mtest_config\u001b[39m.\u001b[39;49mat[test_idx,\u001b[39m'\u001b[39;49m\u001b[39mElastic_Weights\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     27\u001b[0m         turn_penalty\u001b[39m=\u001b[39;49mtest_config\u001b[39m.\u001b[39;49mat[test_idx,\u001b[39m'\u001b[39;49m\u001b[39mTurns\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     28\u001b[0m         path_exposure_attribute\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     29\u001b[0m         return_path_record\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \n\u001b[0;32m     30\u001b[0m         destniation_cap\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m return_dict\n",
      "File \u001b[1;32mc:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\..\\madina\\una\\betweenness.py:486\u001b[0m, in \u001b[0;36mbetweenness_exposure\u001b[1;34m(self, core_index, origin_queue, search_radius, detour_ratio, decay, beta, decay_method, path_detour_penalty, closest_destination, elastic_weight, turn_penalty, path_exposure_attribute, return_path_record, destniation_cap)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[39m# finding all destination reachible from this origin within a search radius, and finding all paths within a derour ration from the shortest path\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     path_edges, weights, d_idxs \u001b[39m=\u001b[39m path_generator(\n\u001b[0;32m    487\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork,\n\u001b[0;32m    488\u001b[0m         origin_idx,\n\u001b[0;32m    489\u001b[0m         search_radius\u001b[39m=\u001b[39;49msearch_radius,\n\u001b[0;32m    490\u001b[0m         detour_ratio\u001b[39m=\u001b[39;49mdetour_ratio,\n\u001b[0;32m    491\u001b[0m         turn_penalty\u001b[39m=\u001b[39;49mturn_penalty\n\u001b[0;32m    492\u001b[0m     )\n\u001b[0;32m    493\u001b[0m     path_generation_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start\n\u001b[0;32m    494\u001b[0m     start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\..\\madina\\una\\paths.py:17\u001b[0m, in \u001b[0;36mpath_generator\u001b[1;34m(network, o_idx, search_radius, detour_ratio, turn_penalty)\u001b[0m\n\u001b[0;32m     14\u001b[0m network\u001b[39m.\u001b[39madd_node_to_graph(o_graph, o_idx)\n\u001b[0;32m     15\u001b[0m \u001b[39m#network.update_light_graph(o_graph, add_nodes=[o_idx])\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m d_idxs, o_scope, o_scope_paths \u001b[39m=\u001b[39m turn_o_scope(\n\u001b[0;32m     18\u001b[0m     network\u001b[39m=\u001b[39;49mnetwork,\n\u001b[0;32m     19\u001b[0m     o_idx\u001b[39m=\u001b[39;49mo_idx,\n\u001b[0;32m     20\u001b[0m     search_radius\u001b[39m=\u001b[39;49msearch_radius,\n\u001b[0;32m     21\u001b[0m     detour_ratio\u001b[39m=\u001b[39;49mdetour_ratio,\n\u001b[0;32m     22\u001b[0m     turn_penalty\u001b[39m=\u001b[39;49mturn_penalty,\n\u001b[0;32m     23\u001b[0m     o_graph\u001b[39m=\u001b[39;49mo_graph,\n\u001b[0;32m     24\u001b[0m     return_paths\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[39m#if len(d_idxs) == 0:\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39m#    # no destinations reachable, return empty dests, paths, weights.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m#    return {}, {}, {}\u001b[39;00m\n\u001b[0;32m     32\u001b[0m scope_nodes, distance_matrix, d_idxs \u001b[39m=\u001b[39m bfs_subgraph_generation(\n\u001b[0;32m     33\u001b[0m     o_idx\u001b[39m=\u001b[39mo_idx,\n\u001b[0;32m     34\u001b[0m     detour_ratio\u001b[39m=\u001b[39mdetour_ratio,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     o_scope_paths\u001b[39m=\u001b[39mo_scope_paths,\n\u001b[0;32m     39\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\..\\madina\\una\\paths.py:370\u001b[0m, in \u001b[0;36mturn_o_scope\u001b[1;34m(network, o_idx, search_radius, detour_ratio, turn_penalty, o_graph, return_paths)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[39mTODO: fill out the spec\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[39mo_idx: origin index, integer, coming from the node_gdf\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39mo_graph: reusing updated graphs (e. g. doing inelastic after elastic), optional\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    369\u001b[0m node_gdf \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39mnodes\n\u001b[1;32m--> 370\u001b[0m destinations \u001b[39m=\u001b[39m node_gdf[node_gdf[\u001b[39m\"\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mdestination\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mindex\n\u001b[0;32m    371\u001b[0m \u001b[39m# print(f\"turn_o_scope: {o_idx = }\")\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \n\u001b[0;32m    373\u001b[0m \n\u001b[0;32m    374\u001b[0m \u001b[39m# visualize_graph(self, graph)\u001b[39;00m\n\u001b[0;32m    375\u001b[0m o_scope \u001b[39m=\u001b[39m {o_idx: \u001b[39m0\u001b[39m}\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\madina_dev\\Lib\\site-packages\\geopandas\\geodataframe.py:1415\u001b[0m, in \u001b[0;36mGeoDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m   1410\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1411\u001b[0m \u001b[39m    If the result is a column containing only 'geometry', return a\u001b[39;00m\n\u001b[0;32m   1412\u001b[0m \u001b[39m    GeoSeries. If it's a DataFrame with any columns of GeometryDtype,\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m \u001b[39m    return a GeoDataFrame.\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1415\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(key)\n\u001b[0;32m   1416\u001b[0m     geo_col \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_geometry_column_name\n\u001b[0;32m   1417\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, Series) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(result\u001b[39m.\u001b[39mdtype, GeometryDtype):\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\madina_dev\\Lib\\site-packages\\pandas\\core\\frame.py:3798\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3796\u001b[0m \u001b[39m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   3797\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 3798\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_bool_array(key)\n\u001b[0;32m   3800\u001b[0m \u001b[39m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m \u001b[39m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m is_single_key \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\madina_dev\\Lib\\site-packages\\pandas\\core\\frame.py:3852\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3849\u001b[0m \u001b[39m# check_bool_indexer will throw exception if Series key cannot\u001b[39;00m\n\u001b[0;32m   3850\u001b[0m \u001b[39m# be reindexed to match DataFrame rows\u001b[39;00m\n\u001b[0;32m   3851\u001b[0m key \u001b[39m=\u001b[39m check_bool_indexer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, key)\n\u001b[1;32m-> 3852\u001b[0m indexer \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39;49mnonzero()[\u001b[39m0\u001b[39m]\n\u001b[0;32m   3853\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_take_with_is_copy(indexer, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "def profile_new():\n",
    "    origin_gdf = origin_nodes\n",
    "    num_cores = 1\n",
    "    with mp.Manager() as manager:\n",
    "        #create aNDfill queue\n",
    "        origin_queue = manager.Queue()\n",
    "        for o_idx in origin_gdf.index[:]:\n",
    "            origin_queue.put(o_idx)\n",
    "        for core_index in range(num_cores):\n",
    "            origin_queue.put(\"done\")\n",
    "\n",
    "\n",
    "        return_dict = betweenness_exposure(\n",
    "            harvard_square,\n",
    "            core_index=1,\n",
    "            origin_queue=origin_queue,\n",
    "            #origins=origin_gdf.iloc[:1000],\n",
    "            search_radius=test_config.at[test_idx,'Radius'],\n",
    "            detour_ratio=test_config.at[test_idx,'Detour'],\n",
    "            decay=False if test_config.at[test_idx,'Elastic_Weights'] else test_config.at[test_idx,'Decay'],\n",
    "            beta=test_config.at[test_idx,'Beta'],\n",
    "            decay_method=test_config.at[test_idx,'Decay_Mode'],\n",
    "            path_detour_penalty='equal',\n",
    "            closest_destination=test_config.at[test_idx,'Closest_Destination'],\n",
    "            elastic_weight=test_config.at[test_idx,'Elastic_Weights'],\n",
    "            turn_penalty=test_config.at[test_idx,'Turns'],\n",
    "            path_exposure_attribute=None,\n",
    "            return_path_record=False, \n",
    "            destniation_cap=None\n",
    "    )\n",
    "    return return_dict\n",
    "x = profile_new()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "madina_env_latest_updates",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
