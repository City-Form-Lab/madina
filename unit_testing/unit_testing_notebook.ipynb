{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# email draft\n",
    "\n",
    "Hey Andres, Liu:\n",
    "\n",
    "I finished creating a pedestrian workflow on the new codebase which can now be used to run simulations. Instructions on how to install the new code as well as how to fill the new pairing table are found here: Please read carefully before running new simulation and when updating the pairing table, please pay close attention to warnings highlighted in red, and specific keywords highlighted in grten.\n",
    "\n",
    "Liu: \n",
    "* Please order the rwos in the pairing table such that rwos that use amenities or jobs are at the bottum of the pairing table\n",
    "* please use this new updated subway file as it has an updated column for XXX\n",
    "\n",
    "\n",
    "general library improvements:\n",
    "* better organization of functions into folders. Better object structure that simplifies internal data storage and referencing which improves readibility and makes expansion easier. (Thanks for work done by out UROPs Kwesi and Orion)\n",
    "* each version is now associated with a version number and a release date for better tracking of what version of the code was used to generate what simulation. This information is now added to the first entry of the simulation log\n",
    "\n",
    "updates to the betweenness:\n",
    "* Option to include an exposure parameters, results are reported to the origin layer\n",
    "* option to expore path level data from the betweenness simulation, useful fo9r debugging and manual queries\n",
    "* simplified implimintation of the betweenness that is easier to read and expand\n",
    "* reduction of unnessisary loops and better use of vectorization, putential minor effeciency gains\n",
    "* better and more optimized handiling of elastic weights that avoids repeated calculation. Now is a function call made inside the betweenness, leveraging internal calculations already generated\n",
    "\n",
    "Updates to network creation:\n",
    "* Significiantly faster network creation\n",
    "* Significiantly Faster node insertion\n",
    "* Redundanrt edges are now split into two segments, improving simulation accuricy compared to old behaviour which ignored one optipn\n",
    "\n",
    "Updates to pairing table:\n",
    "* more explicit simulation settings in the pairing table\n",
    "* removal of the nested \"simulation setting\" folder system, and replaced with a flat folder that matches pairing table specifications. No simulation settings are needed when running a simulation as everything is explicitly set in the pairting table\n",
    "\n",
    "Updates to logger:\n",
    "* Added feature to enable storage of origin characteristics like reach, gravity and elasic weight. \n",
    "* enhancements to the flow map include showing OD snapping line\n",
    "* Including the code version number and release date as part of the log\n",
    "* inclufing data file projections in the log to enable quicker debugging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## TODO\n",
    "# TODO\n",
    "* ~~include split redundant edges into code~~\n",
    "* ~~commit to github~~\n",
    "* ~~update the elastic weight function to be at the origin level~~\n",
    "* ~~transfer the betweenness flow with exposure into the new code~~\n",
    "* ~~commit to github~~\n",
    "* ~~write a documentation of the pairing table~~\n",
    "* ~~update the logger and the file saving mechanisim~~\n",
    "* ~~include origin and destination line connectors~~\n",
    "* ~~commit to github~~\n",
    "* ~~move workflow into code, create notebook of workflows~~\n",
    "* ~~commit to githun~~\n",
    "* ~~emale andres and lui~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tuesday 2023-07-11 action items:\n",
    "* ~~Debug Beirut, see where the source of decreased knn weight~~\n",
    "* ~~re run an accessibility simulation, as well as a betweenness flow simulation for beirut~~\n",
    "* ~~Once simulation accessibility results done, send to danie~~ addd flows to folder and send another email\n",
    "* ~~create visuak maos for scenarios showing increase in reach.. Add to google slides~~\n",
    "* ~~Homes to > transit - parks - amenities - schools~~\n",
    "* ~~geometric - no turns, no elastic weight, no decay ....~~\n",
    "* ~~la_extended16_manualFixed~~\n",
    "* ~~Run Somerville~~\n",
    "* ~~RUn LA~~\n",
    "* ~~look at Andres' email about brooklyn , run a simulation using the old code in the workstation~~\n",
    "* ~~look at Lui's email, see if I need to do anything about it~~\n",
    "* ~~create OD pairing table for LA, send to Andres~~\n",
    "* ~~RUN A FKOW MODEL FOR HOMES TO MOSKS, USING THE SAME PARAMETERS, WITH KNN , FOR ALL SCENARIOS, SEND TO DANIEL~~\n",
    "* ~~CLip lui's dataset into the area that daniel chose, share with andres~~\n",
    "*~~ FOR the clip, use this file \"C:\\Users\\abdul\\Dropbox (MIT)\\115_NYCWalks\\03_Data\\02_Edited\\NYC_Sidewalk_Edit_July\\BK_clipping.geojson\"~~\n",
    "* ~~make sure the new node insertion code works and matches well~~\n",
    "\n",
    "## effecient node edge creation\n",
    "* ~~Make sure the functionality to construct nodes-edges works for a safey buffer~~\n",
    "* ~~Isolate the node insertion code into script~~\n",
    "* ~~commit script to my branch~~\n",
    "* ~~email orion about it, mention how easy to integrate.~~\n",
    "* ~~look into issue with redundant edges~~\n",
    "\n",
    "## Testing\n",
    "* ~~structure tests so its a one csv: first n rows are settings, next m rows are edges flows, with first column being same segment ids from network file~~\n",
    "* ~~standard test case is four files: network.jeojson - origins.geojson - destination.jeojson - testflows.csv~~\n",
    "* ~~function reads a csv into list of dicts: test settings and series of output.~~\n",
    "* ~~build Harvard Square testflows.csv~~\n",
    "~~* make sure this config runs on Harvard Square~~\n",
    "\n",
    "\n",
    "\n",
    "# TODO\n",
    "## Functionality List 1\n",
    "* add visualization functionality to new version of Madina so testing is easier\n",
    "* in network, add  connector segments as a separate layer (Maybe not very needed if origins and destinations visually are snapoped...)\n",
    "* Commit visualization updates to github\n",
    "\n",
    "\n",
    "## Testing\n",
    "* digitize three test cases for manhattan, visualize OD snapping, show where differences (if any) come from between Rhino  and Python in the NYC case.\n",
    "* Put maps into slides\n",
    "* CHeck somerville, to see if we delete the same number of edges, with and without the effecient code \"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\Cities\\Somerville\\Data\\Somerville_network_1000m_buffer.geojson\"\n",
    "\n",
    "## Functionality List 2\n",
    "* build the betweenness simulation workflow, combine with accissibility\n",
    "* include Logger as a class, and zonal would have an instance of that class. Logger handles event documentation, and captures output.\n",
    "* for each origin, have an origin record, showing its knn weight, reach, gravity towards each of its destination, header rows showing settings and parameters\n",
    "* for the network, columns of od flows, headed  by settings and parameters rows that explicitly detail units, weight type, calibration status, .. any relevant data, settings and parameters..\n",
    "* set up a slideshow/diagram to show the relationship between different compoonents of the library.\n",
    "* Set up a notebook to go over estimating flows from one origin to one destination, Document all relevant settings\n",
    "* show how this generalizes to the pairings.csv, iterating over moodel settings, iterating over scenarios\n",
    "* Make a finalized github commit including testing script, betweenness flow notebook. \n",
    "* Deploy to workstation.\n",
    "\n",
    "\n",
    "## Functionality List 3\n",
    "* given the simulation results, and the censor locations, find a way to calibrate counts and generate beta coeffecients (Beirut, SOmerville, NYC)\n",
    "* enable Jupyter notebook server access from workstation.\n",
    "* Figure out a way to do the \"Cities\" dropbox folder, people can add data through dropbox, run simulations through Jupyter Server, get results through Dropbox\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Running Cases\n",
    "* Send output of LA to Nico when done\n",
    "* send Brooklyn output when done\n",
    "* be prepared to run entire NYC simulation, be mindful that daniel already sent more network parts\n",
    "\n",
    "\n",
    "## Map Cleaning\n",
    "https://www.dropbox.com/scl/fo/d6ugcyt3aanfb3yk4n9va/h?dl=0&rlkey=1c2giilvzoyygcxrlry7fscr9\n",
    "\n",
    "\n",
    "\n",
    "! pip install tqdm\n",
    "#from tqdm import tqdm\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_config.index[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\.conda\\envs\\madina_dev\\Lib\\site-packages\\geopandas\\_compat.py:123: UserWarning: The Shapely GEOS version (3.11.2-CAPI-1.17.2) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\..\\madina\\zonal\\network.py:6: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  from geopandas import GeoDataFrame\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test name                      | madina_flow_sum | Rhino_flow_sum  | sum_diff | sum_smlr_% | sle_mean | sle_median | sle_max  | sle>0.1% | sle>1.0% | sle>3.0% | sle>5.0%\n",
      "Somerville_Bus_Subway_Geometri |          700.29 |          713.14 |   -12.84 |    101.80% |  2.3488% |    2.2893% |  5.0852% |      266 |      232 |       96 |        2\n",
      "Somerville_Bus_Subway          |          730.95 |          743.95 |   -13.00 |    101.75% |  2.4117% |    2.2917% |  5.1912% |      285 |      251 |      103 |        8\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "\n",
    "from madina.zonal.zonal import Zonal\n",
    "from madina.una.betweenness import parallel_betweenness\n",
    "from madina.una.elastic import get_elastic_weight\n",
    "from madina.una.betweenness import paralell_betweenness_exposure\n",
    "\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "from geopandas import GeoDataFrame, GeoSeries\n",
    "\n",
    "\n",
    "print (f\"{'test name':30s} | {'madina_flow_sum':15s} | {'Rhino_flow_sum':15s} | {'sum_diff':8s} | {'sum_smlr_%':10s} | {'sle_mean':8s} | {'sle_median':10s} | {'sle_max':8s} | {'sle>0.1%':8s} | {'sle>1.0%':8s} | {'sle>3.0%':8s} | {'sle>5.0%':8s}\")\n",
    "#for test_case in os.listdir(\"Test Cases\"):\n",
    "for test_case in ['Somerville']:   # 'Harvard Square',\n",
    "    # TODO: Check OS compatibility, ensure this is compatible with Unix systems..\n",
    "    test_case_folder = \"Test Cases\" + \"\\\\\" + test_case + \"\\\\\"\n",
    "    test_config = pd.read_csv(test_case_folder + \"test_configs.csv\")\n",
    "    test_flows =  pd.read_csv(test_case_folder + \"test_flows.csv\")\n",
    "\n",
    "    if test_case == 'Somerville':\n",
    "        ready_tests = test_config.index[0:2]\n",
    "    else:\n",
    "        ready_tests = test_config.index\n",
    "\n",
    "    for test_idx in ready_tests:\n",
    "\n",
    "        harvard_square = Zonal()\n",
    "\n",
    "        harvard_square.load_layer(\n",
    "            layer_name='streets',\n",
    "            file_path=  test_case_folder + test_config.at[test_idx, 'Network_File']\n",
    "            )\n",
    "\n",
    "        harvard_square.load_layer(\n",
    "            layer_name=test_config.at[test_idx, 'Origin_Name'],\n",
    "            file_path= test_case_folder + test_config.at[test_idx, 'Origin_File']\n",
    "            )\n",
    "\n",
    "        harvard_square.load_layer(\n",
    "            layer_name=test_config.at[test_idx, 'Destination_Name'],\n",
    "            file_path= test_case_folder + test_config.at[test_idx, 'Destination_File']\n",
    "            )\n",
    "        \n",
    "\n",
    "        harvard_square.create_street_network(\n",
    "            source_layer='streets', \n",
    "            discard_redundant_edges=False,\n",
    "            node_snapping_tolerance=0.01,  # TODO: check for sensitivity... pick one as default snapping.\n",
    "            weight_attribute=test_config.at[test_idx, 'Network_Cost'] if test_config.at[test_idx, 'Network_Cost'] != \"Geometric\" else None\n",
    "        )\n",
    "\n",
    "        #print (f\"{harvard_square.network.edges.shape = }\")\n",
    "        #print (f\"{harvard_square.network.nodes.shape = }\")\n",
    "        #harvard_square.network.nodes,  harvard_square.network.edges = _discard_redundant_edges(harvard_square.network.nodes, harvard_square.network.edges)\n",
    "        #print (f\"{harvard_square.network.edges.shape = }\")\n",
    "        #print (f\"{harvard_square.network.nodes.shape = }\")\n",
    "\n",
    "        harvard_square.insert_node(\n",
    "            layer_name=test_config.at[test_idx, 'Origin_Name'], \n",
    "            label='origin', \n",
    "            weight_attribute=test_config.at[test_idx, 'Origin_Weight'] if test_config.at[test_idx, 'Origin_Weight'] != \"Count\" else None\n",
    "        )\n",
    "        harvard_square.insert_node(\n",
    "            layer_name=test_config.at[test_idx, 'Destination_Name'], \n",
    "            label='destination', \n",
    "            weight_attribute=test_config.at[test_idx, 'Destination_Weight'] if test_config.at[test_idx, 'Destination_Weight'] != \"Count\" else None\n",
    "        )\n",
    "\n",
    "        harvard_square.create_graph()\n",
    "\n",
    "        node_gdf = harvard_square.network.nodes\n",
    "        origin_gdf = node_gdf[node_gdf['type'] == 'origin']\n",
    "\n",
    "        harvard_square.network.nodes[\"original_weight\"] = harvard_square.network.nodes[\"weight\"]\n",
    "\n",
    "        harvard_square.network.turn_penalty_amount = test_config.at[test_idx, 'Turn_Penalty']\n",
    "        harvard_square.network.turn_threshold_degree = test_config.at[test_idx, 'Turn_Threshold']\n",
    "\n",
    "        if test_config.at[test_idx, 'Elastic_Weights']:\n",
    "            '''\n",
    "            harvard_square.network.nodes[\"weight\"] = harvard_square.network.nodes[\"original_weight\"]\n",
    "            get_elastic_weight(\n",
    "                harvard_square.network,\n",
    "                search_radius=test_config.at[test_idx, 'Radius'],\n",
    "                detour_ratio=test_config.at[test_idx, 'Detour'],\n",
    "                beta=test_config.at[test_idx, 'Beta'],\n",
    "                decay=True, #test_config.at[test_idx, 'Decay'],\n",
    "                #turn_penalty=test_config.at[test_idx, 'Turns'],\n",
    "                turn_penalty=False,\n",
    "            )\n",
    "            for o_idx in origin_gdf.index:\n",
    "                harvard_square.network.nodes.at[o_idx, 'weight'] =  harvard_square.network.nodes.at[o_idx, 'elastic_weight']\n",
    "            '''\n",
    "            continue\n",
    "        '''\n",
    "        return_dict = parallel_betweenness(\n",
    "            harvard_square.network,\n",
    "            search_radius=test_config.at[test_idx, 'Radius'],\n",
    "            detour_ratio=test_config.at[test_idx, 'Detour'],\n",
    "            decay=test_config.at[test_idx, 'Decay'], #if test['Elastic weights'] else True,\n",
    "            decay_method=test_config.at[test_idx, 'Decay_Mode'],  # \"power\", \"exponent\"\n",
    "            beta=test_config.at[test_idx, 'Beta'],\n",
    "            path_detour_penalty='equal', # \"equal\",  # \"power\", \"exponent\", \"equal\"\n",
    "            origin_weights=False if type(test_config.at[test_idx, 'Origin_Weight']) != str else True,\n",
    "            closest_destination=test_config.at[test_idx, 'Closest_Destination'],\n",
    "            destination_weights=False if type(test_config.at[test_idx, 'Destination_Weight']) != str  else True,    #or (test['Elastic weights'])\n",
    "            # perceived_distance=False,\n",
    "            num_cores=6,\n",
    "            light_graph=True,\n",
    "            turn_penalty=test_config.at[test_idx, 'Turns'],\n",
    "        )\n",
    "        \n",
    "        '''\n",
    "        return_dict = paralell_betweenness_exposure(\n",
    "            harvard_square,\n",
    "            search_radius=test_config.at[test_idx,'Radius'],\n",
    "            detour_ratio=test_config.at[test_idx,'Detour'],\n",
    "            decay=False if test_config.at[test_idx,'Elastic_Weights'] else test_config.at[test_idx,'Decay'],  # elastic weight already reduces origin weight factoring in decay. if this pairing uses elastic weights, don't decay again,\n",
    "            decay_method=test_config.at[test_idx,'Decay_Mode'],\n",
    "            beta=test_config.at[test_idx,'Beta'],\n",
    "            num_cores=6,\n",
    "            path_detour_penalty='equal', # \"power\" | \"exponent\" | \"equal\"\n",
    "            closest_destination=test_config.at[test_idx,'Closest_Destination'],\n",
    "            elastic_weight=test_config.at[test_idx,'Elastic_Weights'],\n",
    "            turn_penalty=test_config.at[test_idx,'Turns'],\n",
    "            path_exposure_attribute=None,\n",
    "            return_path_record=False, \n",
    "            destniation_cap=None\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        simulated_sum_of_flow = return_dict['edge_gdf']['betweenness'].sum()\n",
    "        test_flow = test_flows[test_config.at[test_idx, 'Flow_Name']].sum()\n",
    "\n",
    "\n",
    "        ## create segment level comparison\n",
    "        # creating connector lines\n",
    "\n",
    "        import shapely.geometry as geo\n",
    "        simulated_betweenness = return_dict['edge_gdf'][['betweenness', 'parent_street_id']].rename(columns={\"betweenness\": \"simulated_betweenness\"}).drop_duplicates(subset=['parent_street_id']).set_index(\"parent_street_id\")\n",
    "        simulated_betweenness = harvard_square.layers[\"streets\"].gdf[[\"geometry\", \"__GUID\"]].join(simulated_betweenness).set_index(\"__GUID\")\n",
    "\n",
    "        test_name = test_config.at[test_idx, 'Flow_Name']\n",
    "        test_betweenness = test_flows[['__GUID', test_name]].set_index(\"__GUID\").rename(columns = {test_name: \"test_flow\"})\n",
    "\n",
    "\n",
    "        comparison = simulated_betweenness.join(test_betweenness)\n",
    "        comparison[\"difference\"] = comparison[\"simulated_betweenness\"] - comparison[\"test_flow\"]\n",
    "        comparison[\"difference_pct\"] = abs(comparison[\"difference\"]) / comparison[\"simulated_betweenness\"] *100\n",
    "        # segment level error\n",
    "        sle = comparison[~comparison[\"difference_pct\"].isna()]['difference_pct']\n",
    "\n",
    "        #'''\n",
    "        node_gdf = harvard_square.network.nodes\n",
    "        edge_gdf = harvard_square.network.edges\n",
    "\n",
    "\n",
    "        origin_layer = harvard_square.layers[test_config.at[test_idx, 'Origin_Name']].gdf\n",
    "        origin_nodes = node_gdf[node_gdf['type'] == 'origin']\n",
    "        origin_joined = origin_layer.join(origin_nodes.set_index('source_id'),lsuffix='_origin')\n",
    "        origin_joined['connector_line'] = origin_joined.apply(lambda x:geo.LineString([x['geometry'], x[\"geometry_origin\"]]), axis=1)\n",
    "        origin_joined[\"geometry\"] = origin_joined['connector_line']\n",
    "\n",
    "\n",
    "        destination_layer = harvard_square.layers[test_config.at[test_idx, 'Destination_Name']].gdf\n",
    "        destination_nodes = node_gdf[node_gdf['type'] == 'destination']\n",
    "        destination_joined = destination_layer.join(destination_nodes.set_index('source_id'),lsuffix='_destination')\n",
    "        destination_joined['connector_line'] = destination_joined.apply(lambda x:geo.LineString([x['geometry'], x[\"geometry_destination\"]]), axis=1)\n",
    "        destination_joined[\"geometry\"] = destination_joined['connector_line']\n",
    "\n",
    "        streets = harvard_square.layers[\"streets\"].gdf \n",
    "        network_file = gpd.read_file(test_case_folder + test_config.at[test_idx, 'Network_File'], engine='pyogrio')\n",
    "\n",
    "\n",
    "        flow_difference = comparison[\n",
    "            ((comparison['test_flow' ] > 0) & (comparison['simulated_betweenness'] == 0)) | \n",
    "            ((comparison['test_flow' ] == 0) & (comparison['simulated_betweenness'] > 0))\n",
    "        ]\n",
    "        harvard_square.create_map(\n",
    "            [\n",
    "                #{'gdf': streets[streets[test_config.at[test_idx, 'Network_Cost']] > 0], 'color': [255, 255, 255], 'text': test_config.at[test_idx, 'Network_Cost']},\n",
    "                {'gdf': streets, 'color': [100, 100, 100], 'opacity': 0.1},\n",
    "                {'gdf': edge_gdf[edge_gdf['betweenness'] > 0], 'color': ['125, 125, 0'], 'text': 'betweenness', 'opacity': 0.2},\n",
    "                #{'gdf': comparison[abs(comparison[\"difference\"]) > 0.01], 'color_by_attribute': 'difference', 'color_method': 'gradient', 'text': 'difference'},\n",
    "                {'gdf': comparison[(comparison[\"difference_pct\"] >= 0.1) & (comparison[\"difference_pct\"] < 100)], 'color_by_attribute': 'difference_pct', 'color_method': 'gradient', 'text': 'difference_pct'},\n",
    "                {'gdf': edge_gdf[edge_gdf['snapped'] == True], 'color': [255, 0, 255], 'opacity': 0.2},\n",
    "                {'gdf': network_file[network_file[\"geometry\"].geom_type == 'Polygon'], 'color': [125, 0, 125], 'text': '__GUID'},\n",
    "                {'gdf': flow_difference, 'color': [255, 255, 0] , 'text': 'difference'},        \n",
    "                #{'gdf': comparison[comparison['difference'] != 0], 'color_by_attribute': 'difference', 'color_method': 'gradient', 'text': 'difference', 'opacity':  0.1},\n",
    "                {'gdf': origin_layer, 'color': [0, 0, 255]},\n",
    "                {'gdf': origin_joined[['geometry']], 'color': [0, 0, 255]},\n",
    "                {'gdf': destination_layer, 'color': [255, 0, 0]},\n",
    "                {'gdf': destination_joined[['geometry']], 'color': [255, 0, 0]},\n",
    "                {'gdf': harvard_square.network.nodes[['geometry', 'type', 'weight']].reset_index(), 'color': [255, 0, 255], 'text': 'id'},\n",
    "            ],\n",
    "            save_as=\"Test Cases\\\\\" + test_case + '\\\\'  + test_name + \"._difference_map.html\"\n",
    "        )\n",
    "        #'''\n",
    "        #print (test_config.loc[test_idx])\n",
    "        print (f\"{test_config.at[test_idx, 'Flow_Name'][:30]:30s} | {simulated_sum_of_flow:15.2f} | {test_flow:15.2f} | {simulated_sum_of_flow - test_flow:8.2f} | {1-(simulated_sum_of_flow - test_flow)/ test_flow:10.2%} | {sle.mean():7.4f}% | {sle.median():9.4f}% | {sle.max():7.4f}% | {sle[sle > 0.1].count():8} | {sle[sle > 1.0].count():8} | {sle[sle > 3.0].count():8} | {sle[sle > 5.0].count():8}\")\n",
    "        #print (\"DOne Case...\")\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### paralell_betweenness_exposure\n",
    "test name                      | madina_flow_sum | Rhino_flow_sum  | sum_diff | sum_smlr_% | sle_mean | sle_median | sle_max  | sle>0.1% | sle>1.0% | sle>3.0% | sle>5.0%\n",
    "Somerville_Bus_Subway_Geometri |          700.29 |          713.14 |   -12.84 |    101.80% |  2.3488% |    2.2893% |  5.0852% |      266 |      232 |       96 |        2\n",
    "Somerville_Bus_Subway          |          730.95 |          743.95 |   -13.00 |    101.75% |  2.4117% |    2.2917% |  5.1912% |      285 |      251 |      103 |        8\n",
    "### parallel_betweenness\n",
    "\n",
    "\n",
    "\n",
    "### parallel_betweenness\n",
    "test name                      | madina_flow_sum | Rhino_flow_sum  | sum_diff | sum_smlr_% | sle_mean | sle_median | sle_max  | sle>0.1% | sle>1.0% | sle>3.0% | sle>5.0%\n",
    "_Betweenness1                  |          704.85 |          702.07 |     2.78 |     99.60% |  0.0376% |    0.0000% |  2.4893% |        2 |        2 |        0 |        0\n",
    "_Betweenness2                  |        19116.27 |        19025.63 |    90.64 |     99.52% |  0.0439% |    0.0000% |  3.1837% |        2 |        2 |        1 |        0\n",
    "_Betweenness3                  |        19299.00 |        19214.00 |    85.00 |     99.56% |  0.0295% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0\n",
    "_Betweenness4                  |        20938.18 |        20853.18 |    85.00 |     99.59% |  0.0206% |    0.0000% |  2.7402% |        1 |        1 |        0 |        0\n",
    "Somerville_Bus_Subway_Geometri |          709.44 |          713.14 |    -3.69 |    100.52% |  0.5027% |    0.1954% |  3.9604% |      149 |       62 |        3 |        0\n",
    "Somerville_Bus_Subway          |          740.19 |          743.95 |    -3.76 |    100.51% |  0.5074% |    0.2539% |  3.9604% |      173 |       39 |        3 |        0\n",
    "Somerville_Homes_Subway        |       377922.37 |       389383.31 | -11460.94 |    102.94% |     inf% |    3.8409% |     inf% |     2578 |     2232 |     1591 |      997\n",
    "Somerville_Jobs_Subway         |       146309.13 |       149908.30 | -3599.17 |    102.40% |     inf% |    3.5776% |     inf% |     2078 |     1741 |     1286 |      824"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COnstructing the Manhattan Case..\n",
    "## Don't forget to check for node insertion, set  return_all=False in the spatial index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "buildings_file = r\"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\Test Cases\\Manhattan\\Home_PT_6538.geojson\"\n",
    "subway_file = r\"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\Test Cases\\Manhattan\\Metro_PT_6538.geojson\"\n",
    "network_file = r\"C:\\Users\\abdul\\Dropbox (MIT)\\PhD Thesis\\Madina\\madina\\unit_testing\\Test Cases\\Manhattan\\network_clipped_dupremovedAS.geojson\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from madina.zonal.zonal import Zonal\n",
    "\n",
    "\n",
    "harvard_square = Zonal()\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t imports done, object created\")\n",
    "start = time.time()\n",
    "\n",
    "#harvard_square.load_layer(\n",
    "#    layer_name='streets',\n",
    "#    file_path=network_file\n",
    "#    )\n",
    "\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t street data loaded\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.load_layer(\n",
    "    layer_name='buildings',\n",
    "    file_path=buildings_file\n",
    "    )\n",
    "\n",
    "buildings_gdf = harvard_square.layers['buildings'].gdf\n",
    "harvard_square.layers['buildings'].gdf = buildings_gdf[~buildings_gdf['FID'].isin([27967, 9140, 3974])]\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t building data loaded\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.load_layer(\n",
    "    layer_name='subway',\n",
    "    file_path=subway_file\n",
    "    )\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t subway data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "harvard_square.create_street_network(\n",
    "    source_layer='streets', \n",
    "    discard_redundant_edges=False, \n",
    "    node_snapping_tolerance=0\n",
    ")\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t street network created\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.insert_node(\n",
    "    layer_name='buildings', \n",
    "    label='origin', \n",
    "    weight_attribute='TotalPop'\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t origins insertes\")\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.insert_node(\n",
    "    layer_name='subway', \n",
    "    label='destination', \n",
    "    weight_attribute='line_ent_st'\n",
    ")\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t destinations insertes\")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "harvard_square.create_graph(light_graph=True, d_graph=True)\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t graph created insertes\")\n",
    "start = time.time()\n",
    "\n",
    "return_dict = parallel_betweenness(\n",
    "    harvard_square.network,\n",
    "    search_radius=800,\n",
    "    detour_ratio=1.15,\n",
    "    decay=False,\n",
    "    decay_method='exponent',  # \"power\", \"exponent\"\n",
    "    beta=0.004,\n",
    "    path_detour_penalty=\"equal\",  # \"power\", \"exponent\", \"equal\"\n",
    "    origin_weights=True,\n",
    "    closest_destination=False,\n",
    "    destination_weights=True, \n",
    "    # perceived_distance=False,\n",
    "    num_cores=8,\n",
    "    light_graph=True,\n",
    "    turn_penalty=False,\n",
    ")\n",
    "simulated_sum_of_flow = return_dict['edge_gdf']['betweenness'].sum()\n",
    "\n",
    "print(f\"{(time.time()-start)*1000:6.2f}ms\\t Betweenness estimated\")\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_results = harvard_square.layers['streets'].gdf.join(harvard_square.network.edges[['parent_street_id', 'betweenness']].set_index('parent_street_id'))\n",
    "joined_results[['__GUID', 'betweenness', 'geometry']].to_csv('2023-07-07 manhattan betweenness flow test.csv')\n",
    "joined_results[['__GUID', 'betweenness', 'geometry']].to_file('2023-07-07 manhattan betweenness flow test.geoJSON', driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard_square.create_map(\n",
    "    [\n",
    "        #{\n",
    "            #'layer': 'streets',\n",
    "            #'color': [125, 125, 125],\n",
    "        #},\n",
    "        {\n",
    "            'gdf': harvard_square.layers['homes'].gdf,\n",
    "            'color_by_attribute': 'line_ent_st',\n",
    "            \"color_method\": 'gradient'\n",
    "        }\n",
    "    ],\n",
    "    save_as=\"map.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to instal pydeck jupyter nb extension\n",
    "\n",
    "`jupyter nbextension install --sys-prefix --symlink --overwrite --py pydeck`\n",
    "\n",
    "`jupyter nbextension enable --sys-prefix --py pydeck`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "madina_env_latest_updates",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
